import React, { useState, useEffect, useRef } from 'react';
import { useMicVAD } from '@ricky0123/vad-react';

interface VoiceInputProps {
    onSend: (message: string) => void;
    disabled?: boolean;
    onSpeechStart?: () => void; // ç”¨æˆ·å¼€å§‹è¯´è¯æ—¶çš„å›è°ƒï¼ˆç”¨äºä¸­æ–­ TTSï¼‰
}

// è¾…åŠ©å‡½æ•°ï¼šFloat32Array -> Int16Array
const floatTo16BitPCM = (input: Float32Array) => {
    const output = new Int16Array(input.length);
    for (let i = 0; i < input.length; i++) {
        const s = Math.max(-1, Math.min(1, input[i]));
        output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return output;
};

const VoiceInput: React.FC<VoiceInputProps> = ({ onSend, disabled, onSpeechStart }) => {
    const [transcribing, setTranscribing] = useState(false);
    const [error, setError] = useState<string>('');
    const [transcript, setTranscript] = useState<string>('');

    const wsRef = useRef<WebSocket | null>(null);
    const onSendRef = useRef(onSend);

    // Keep onSendRef current
    useEffect(() => {
        onSendRef.current = onSend;
    }, [onSend]);

    useEffect(() => {
        let ws: WebSocket | null = null;
        const connectWS = async () => {
            try {
                const wsUrl = await (window as any).stt.getWSUrl();
                ws = new WebSocket(wsUrl);
                wsRef.current = ws;

                ws.onopen = () => {
                    console.log('[VoiceInput] WebSocket connected');
                    setError('');
                };

                ws.onmessage = (event) => {
                    const data = JSON.parse(event.data);

                    // å¤„ç†æµå¼ STT çš„éƒ¨åˆ†ç»“æœ
                    if (data.type === 'partial') {
                        // å®æ—¶æ˜¾ç¤ºéƒ¨åˆ†è½¬å½•ç»“æœ
                        console.log('[STT] Partial:', data.segment);
                        setTranscript(data.text);
                        // ä¸è®¾ç½® transcribing=falseï¼Œç»§ç»­ç­‰å¾…æœ€ç»ˆç»“æœ

                    } else if (data.type === 'transcript' || data.type === 'transcription') {
                        // å…¼å®¹æ—§æ ¼å¼å’Œæ–°æ ¼å¼
                        console.log('[STT] Final:', data.text);
                        setTranscribing(false);

                        if (data.text.trim()) {
                            setTranscript(data.text);
                            setTimeout(() => {
                                // Use ref here
                                onSendRef.current(data.text);
                                setTranscript('');
                            }, 500);
                        }
                    } else if (data.type === 'error') {
                        setError(data.message);
                        setTranscribing(false);
                    }
                };

                ws.onclose = () => {
                    console.log('[VoiceInput] WebSocket closed');
                };

                ws.onerror = () => {
                    setError('æ— æ³•è¿æ¥è¯­éŸ³æœåŠ¡');
                };

            } catch (e) {
                console.error(e);
                setError('åˆå§‹åŒ–å¤±è´¥');
            }
        };

        connectWS();

        return () => {
            if (ws) ws.close();
        };
    }, []); // Empty dependency array = connect once

    const vad = useMicVAD({
        startOnLoad: true,
        positiveSpeechThreshold: 0.8,
        // minSpeechFrames å·²è¿‡æ—¶ï¼Œç§»é™¤æˆ–ä½¿ç”¨ minSpeechMs (å¦‚æœä¸ç¡®å®šå±æ€§åï¼Œç›´æ¥ç”¨é»˜è®¤å€¼å³å¯)
        // æ ¹æ®æŠ¥é”™æç¤ºï¼Œå¯èƒ½æ˜¯ minSpeechMsï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬å…ˆçœç•¥ï¼Œä½¿ç”¨é»˜è®¤å€¼

        workletURL: "/vad.worklet.bundle.min.js",
        modelURL: "/silero_vad_v5.onnx",
        ortConfig(ort: any) {
            ort.env.wasm.wasmPaths = "/";
            ort.env.wasm.numThreads = 1;
            ort.env.wasm.proxy = false;
        },
        onSpeechStart: () => {
            console.log('Speech started');
            setTranscript('');
            // è§¦å‘ä¸­æ–­ TTS çš„å›è°ƒ
            if (onSpeechStart) {
                onSpeechStart();
            }
        },
        onSpeechEnd: (audio: Float32Array) => {
            console.log('Speech ended, sending audio...', audio.length);
            if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
                setTranscribing(true);
                // ä½¿ç”¨è‡ªå®šä¹‰è½¬æ¢å‡½æ•°
                const pcm16 = floatTo16BitPCM(audio);
                wsRef.current.send(pcm16.buffer);
            }
        },
        onVADMismatch: () => {
            console.warn('VAD Mismatch');
        }
    } as any);

    const getStatusText = () => {
        if (vad.loading) return 'Loading VAD...';
        if (vad.errored) return 'VAD Error';
        if (error) return error;
        if (transcribing) return 'Thinking...';
        if (vad.userSpeaking) return 'Listening...';
        return 'Ready';
    };

    const getIconColor = () => {
        if (error || vad.errored) return '#ff6b6b';
        if (transcribing) return '#ffa502';
        if (vad.userSpeaking) return '#ff4757';
        return 'rgba(255,255,255,0.6)';
    };

    return (
        <div style={{
            position: 'absolute',
            bottom: '30px',
            left: '50%',
            transform: 'translateX(-50%)',
            zIndex: 10,
            display: 'flex',
            flexDirection: 'column',
            alignItems: 'center',
            gap: 10
        }}>
            <div style={{
                width: '60px',
                height: '60px',
                borderRadius: '50%',
                border: `3px solid ${getIconColor()} `,
                backgroundColor: 'rgba(0,0,0,0.7)',
                display: 'flex',
                justifyContent: 'center',
                alignItems: 'center',
                fontSize: '28px',
                boxShadow: vad.userSpeaking ? `0 0 20px ${getIconColor()} ` : '0 4px 10px rgba(0,0,0,0.3)',
                transition: 'all 0.2s ease',
                transform: vad.userSpeaking ? 'scale(1.1)' : 'scale(1)',
            }}>
                {transcribing ? 'â³' : vad.userSpeaking ? 'ğŸ¤' : 'âšª'}
            </div>

            <div style={{
                color: 'white',
                fontSize: '14px',
                textShadow: '0 1px 2px black',
                fontWeight: 500,
                height: '20px'
            }}>
                {getStatusText()}
            </div>

            {transcript && (
                <div style={{
                    maxWidth: '300px',
                    padding: '8px 12px',
                    backgroundColor: 'rgba(0,0,0,0.7)',
                    borderRadius: '8px',
                    color: 'white',
                    fontSize: '12px',
                    marginTop: '5px'
                }}>
                    {transcript}
                </div>
            )}
        </div>
    );
};

export default VoiceInput;
