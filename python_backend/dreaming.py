"""
Dreaming System (ReM - Recursive Episodic Memory)
Handles memory extraction and consolidation for per-character isolation.
"""
import json
import os
import logging
from typing import List, Dict, Any
from datetime import datetime

# Conditional imports
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

logger = logging.getLogger("Dreaming")


class Dreaming:
    """
    The Dreaming System (ReM - Recursive Episodic Memory).
    Cycle: Raw Logs -> Extract -> Active Memories -> (Hit) -> Pending -> Consolidate -> Active
    
    IMPORTANT: Each instance is scoped to a SINGLE character_id.
    For multi-character support, create separate instances per character.
    """
    
    def __init__(self, memory_client=None, character_id: str = "default"):
        """
        Initialize Dreaming for a specific character.
        
        Args:
            memory_client: SurrealMemory instance (shared DB connection)
            character_id: The character this dreaming instance is for (MUST be specified)
        """
        self.memory = memory_client
        self.character_id = character_id.lower()  # Normalize for consistency
        
        # LLM Config
        self.api_key = os.environ.get("OPENAI_API_KEY", "")
        self.base_url = os.environ.get("OPENAI_BASE_URL", "http://localhost:11434/v1")
        self.model = os.environ.get("LLM_MODEL", "deepseek-chat")
        
        # Load config overrides
        self._load_config()
        
        # Initialize LLM client
        self.llm_client = None
        if OpenAI and self.api_key:
            self.llm_client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        
        # ‚ö° Soul Evolution ÈÖçÁΩÆ
        self.soul_evolution_config = {
            "min_interval_minutes": 30,       # ‰∏§Ê¨°ÊºîÂåñÈó¥ÈöîËá≥Â∞ë 30 ÂàÜÈíü
            "min_memories_threshold": 20,     # Ëá≥Â∞ëÂ§ÑÁêÜ 20 Êù°Êñ∞ËÆ∞ÂøÜÂêéËß¶Âèë
            "min_text_length": 500,           # ÂàÜÊûêÊñáÊú¨Ëá≥Â∞ë 500 Â≠óÁ¨¶
        }
        self._last_soul_evolution_time: datetime = None
        self._processed_memories_since_evolution: int = 0
        self._accumulated_text_for_evolution: str = ""
        
        # ‚ö° Soul Manager (Âª∂ËøüÂä†ËΩΩÔºåÈÅøÂÖçÂæ™ÁéØÂØºÂÖ•)
        self._soul_manager = None
        
        logger.info(f"[Dreaming] Initialized for character: {self.character_id}")

    def _load_config(self):
        """Load LLM config from memory_config.json if available."""
        # Use absolute path based on file location
        base_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(base_dir, "memory_config.json")
        
        if os.path.exists(config_path):
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    config = json.load(f)
                    if config.get("base_url"): self.base_url = config["base_url"]
                    if config.get("api_key"): self.api_key = config["api_key"]
                    if config.get("model"): self.model = config["model"]
            except Exception as e:
                logger.warning(f"Failed to load memory_config.json at {config_path}: {e}")


    async def process_memories(self, batch_size: int = 10):
        """
        Main entry point. Runs extraction and consolidation for THIS character only.
        
        ÊµÅÁ®ã:
        1. Extractor: Â§ÑÁêÜ batch_size Êù°Êú™Â§ÑÁêÜÁöÑÂØπËØùÊó•Âøó
        2. Consolidator: Â§ÑÁêÜ BatchManager ‰∏≠ÁöÑÂæÖÊï¥ÂêàÊâπÊ¨°ÔºàÁî± search_hybrid ÂàõÂª∫Ôºâ
        3. Soul Evolution: Êª°Ë∂≥Êù°‰ª∂Êó∂ÂàÜÊûêÂπ∂Êõ¥Êñ∞ÊÄßÊ†º
        
        Args:
            batch_size: Extractor ÊØèÊ¨°Â§ÑÁêÜÁöÑÂØπËØùÊó•ÂøóÊù°Êï∞
        """
        if not self.memory or not self.memory.db:
            logger.error(f"[Dreaming] No database connection for {self.character_id}")
            return
            
        logger.debug(f"[Dreaming] ‚ú® Starting Reverie Cycle for character '{self.character_id}'...")
        
        # Phase 1: Extract raw logs -> memories
        await self._run_extractor(limit=batch_size)
        
        # Phase 2: Consolidate frequently retrieved memories (Hit-Count Based)
        await self._run_consolidator(limit=10)
        
        # Phase 3: Soul Evolution (Êù°‰ª∂Ëß¶Âèë)
        await self._check_and_trigger_soul_evolution()


    # ==================== Phase 1: Extractor ====================
    
    async def _run_extractor(self, limit: int = 10):
        """
        Phase 1: conversation_log -> episodic_memory
        
        Reads raw conversation logs for THIS character only,
        extracts meaningful facts, and stores as active memories.
        """
        # 1. First check total unprocessed count
        count_query = """
        SELECT count() FROM conversation_log 
        WHERE character_id = $character_id 
          AND is_processed = false GROUP ALL;
        """
        count_result = await self.memory.db.query(count_query, {"character_id": self.character_id})
        
        total_count = 0
        if count_result and isinstance(count_result, list):
            first = count_result[0]
            if isinstance(first, dict) and 'result' in first:
                 # result might be [{'count': 25}]
                 inner = first['result']
                 if inner and isinstance(inner, list) and inner and 'count' in inner[0]:
                     total_count = inner[0]['count']
            elif isinstance(first, dict) and 'count' in first:
                 # Direct format: [{'count': 45}]
                 total_count = first['count']
        
        # DEBUG: Print Limit and Count
        logger.info(f"[Dreaming] Extractor Check: Count={total_count}, Limit={limit}, Threshold=20")

        if total_count < 20:
             logger.debug(f"[Dreaming] Accumulating logs for {self.character_id} ({total_count}/20)...")
             return

        # 2. Fetch limit (batch_size) logs
        query = """
        SELECT * FROM conversation_log 
        WHERE character_id = $character_id 
          AND is_processed = false 
        ORDER BY created_at ASC
        LIMIT $limit;
        """
        results = await self.memory.db.query(query, {
            "character_id": self.character_id,
            "limit": limit
        })
        
        # Parse results
        logs = []
        if results and isinstance(results, list):
            first = results[0]
            if isinstance(first, dict) and 'result' in first:
                logs = first['result'] or []
            elif isinstance(first, dict):
                logs = results
            
        if not logs:
            logger.debug(f"[Dreaming] No raw logs to extract for {self.character_id}")
            return

        # DEBUG: Print IDs of fetched logs
        log_ids = [l.get('id', 'unknown') for l in logs]
        logger.info(f"[Dreaming] Extractor Fetched {len(logs)} logs: {log_ids}")

        # Prepare prompt input
        log_text = ""
        for log in logs:
            ts = log.get('created_at', '')[:16].replace('T', ' ')
            narrative = log.get('narrative', '')
            log_text += f"[{ts}] {narrative}\n"
            
        # LLM Prompt
        prompt = f"""‰Ω†ÊòØÊ†∏ÂøÉËÆ∞ÂøÜÊèêÂèñÊ®°Âùó„ÄÇ

### ‰ªªÂä°Ôºö
‰ªéÂØπËØùÊó•Âøó‰∏≠ÊèêÂèñÊúâ‰ª∑ÂÄºÁöÑ‰∫ãÂÆûÔºåÂπ∂ËøõË°åÂèëÊï£ËÅîÊÉ≥„ÄÇ
Ê≥®ÊÑèÔºö
1. ÂØπËØùÊó•ÂøóÊòØÁî±ËØ≠Èü≥ËΩ¨ÂΩïÁîüÊàêÁöÑÔºåÂõ†Ê≠§ÂèØËÉΩÂ≠òÂú®ÈîôÂà´Â≠óÊàñË∞êÈü≥Â≠ó‰ª•ÂèäÊó†ÊÑè‰πâÁöÑÈîô‰π±ÊñáÂ≠óÔºåËØ∑ËøõË°å‰øÆÊ≠£„ÄÇ
2. ÈáçÂ§çÊàñÂÜ≤Á™ÅÁöÑ‰∫ãÂÆûËØ∑Ê†πÊçÆ‰∏ä‰∏ãÊñáËá™Âä®ÂêàÂπ∂ÔºåÂêåÊó∂Â∞ÜÊâÄÊúâÂèØ‰ª•ÂêàÂπ∂ÁöÑÂØπËØùÂêàÂπ∂Êàê‰∏ÄÊù°‰∫ãÂÆû
3. ÂêåÊó∂ÊØèÂè•ÂØπËØù‰∏≠ÂèØËÉΩÂåÖÂê´Â§ö‰∏™‰∏çÂêåÁöÑ‰∏ª‰ΩìÂíå‰∫ãÂÆûÔºåËØ∑ÊääÂÆÉÂàÜÁ¶ªÊàêÂ§ö‰∏™"memory"ÁâáÊÆµ

### ËæìÂá∫Ê†ºÂºè (ÂøÖÈ°ªÊòØÊ†áÂáÜÁöÑ JSON List):
[ 
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì1+‰∫ãÂÆû][ÂØπËÆ∞ÂøÜÁÆÄÁü≠ÁöÑÂèëÊï£ËÅîÊÉ≥]"}},
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì1+‰∫ãÂÆû][ÂØπËÆ∞ÂøÜÁÆÄÁü≠ÁöÑÂèëÊï£ËÅîÊÉ≥]"}},
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì2+‰∫ãÂÆû][ÂØπËÆ∞ÂøÜÁÆÄÁü≠ÁöÑÂèëÊï£ËÅîÊÉ≥]"}},
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì3+‰∫ãÂÆû][ÂØπËÆ∞ÂøÜÁÆÄÁü≠ÁöÑÂèëÊï£ËÅîÊÉ≥]"}},
]
Ê≥®ÊÑèÔºöÂøÖÈ°ªÊòØJSONÊ†ºÂºèÂàóË°®„ÄÇ

[Raw Logs]:
{log_text}
"""
        
        try:
            # Call LLM
            if not self.llm_client:
                logger.warning(f"[Dreaming] No LLM client configured, skipping extraction")
                return
            
            # === DEBUG: ÊâìÂç∞ÂèëÈÄÅÁªô LLM ÁöÑÂÜÖÂÆπ ===
            logger.info(f"[Extractor] üì§ Sending to LLM ({len(logs)} logs):")
            logger.info(f"[Extractor] Prompt:\n{prompt}")
                
            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a memory extractor. Output JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            
            content = response.choices[0].message.content.strip()
            
            # === DEBUG: ÊâìÂç∞ LLM ËøîÂõûÁöÑÂÜÖÂÆπ ===
            logger.info(f"[Extractor] üì• LLM Response received")
            logger.info(f"[Extractor] Raw response:\n{content}")
            
            # Clean markdown wrapper
            if content.startswith("```json"): 
                content = content.split("\n", 1)[1]
            if content.endswith("```"): 
                content = content.rsplit("\n", 1)[0]
            
            new_memories = json.loads(content)
            
            # Ensure it's a list
            if isinstance(new_memories, dict):
                new_memories = new_memories.get("memories", [new_memories])
            
            # Save new memories
            for item in new_memories:
                raw_text = item.get("memory", "")
                if not raw_text: continue
                
                # Generate embedding (384 dim for paraphrase-multilingual-MiniLM-L12-v2)
                vector = [0.0] * 384  # Default dimension
                if hasattr(self.memory, 'encoder') and self.memory.encoder:
                    try:
                        vector = self.memory.encoder(raw_text)
                    except Exception as e:
                        logger.warning(f"Failed to encode memory: {e}")
                
                # Store as active memory (for THIS character)
                await self.memory.add_episodic_memory(
                    character_id=self.character_id,
                    content=raw_text,
                    embedding=vector,
                    status="active"
                )
                
            # Mark logs as processed
            for log in logs:
                log_id = log.get('id', '')
                if log_id:
                    await self.memory.db.query(f"UPDATE {log_id} SET is_processed = true;")
            
            # ‚ö° Á¥ØÁßØÂ§ÑÁêÜÁöÑÂÜÖÂÆπÔºå‰æõ Soul Evolution ‰ΩøÁî®
            self.accumulate_for_evolution(log_text, len(new_memories))
            
            logger.info(f"[Dreaming] Extracted {len(new_memories)} fragments from {len(logs)} logs for '{self.character_id}'")
            
        except json.JSONDecodeError as e:
            logger.error(f"[Dreaming] Failed to parse LLM response as JSON: {e}")
        except Exception as e:
            logger.error(f"[Dreaming] Extractor failed for {self.character_id}: {e}")


    # ==================== Phase 2: Consolidator ====================

    async def _run_consolidator(self, limit: int = 10):
        """
        Phase 2: Consolidate 'active' memories that are frequently retrieved (hit_count > 1).
        Trigger Condition: At least 20 such memories exist.
        Execution: Pick Top N (limit) by hit_count.
        """
        # 1. Check Count of candidates
        count_query = """
        SELECT count() FROM episodic_memory 
        WHERE character_id = $character_id 
          AND status = 'active'
          AND hit_count > 1
        GROUP ALL;
        """
        count_result = await self.memory.db.query(count_query, {"character_id": self.character_id})
        
        candidate_count = 0
        if count_result and isinstance(count_result, list):
             first = count_result[0]
             if isinstance(first, dict) and 'result' in first:
                 inner = first['result']
                 if inner and isinstance(inner, list) and 'count' in inner[0]:
                     candidate_count = inner[0]['count']
                     
        if candidate_count < 20:
             logger.debug(f"[Dreaming] Consolidator skipped: Only {candidate_count}/20 candidates (active & hit>1)")
             return

        logger.info(f"[Dreaming] üß† Consolidator Triggered! Found {candidate_count} candidates. Processing Top {limit}...")

        # 2. Fetch Top N High-Hit Memories
        query = """
        SELECT * FROM episodic_memory 
        WHERE character_id = $character_id 
          AND status = 'active' 
          AND hit_count > 1
        ORDER BY hit_count DESC
        LIMIT $limit;
        """
        results = await self.memory.db.query(query, {
            "character_id": self.character_id,
            "limit": limit
        })
        
        # Parse results
        pending_mems = []
        if results and isinstance(results, list):
            first = results[0]
            if isinstance(first, dict) and 'result' in first:
                pending_mems = first['result'] or []
            elif isinstance(first, dict):
                pending_mems = results
                
        if not pending_mems:
            return

        # ÂáÜÂ§á LLM ËæìÂÖ•
        input_list = []
        for i, mem in enumerate(pending_mems):
            input_list.append({
                "id": str(i + 1),
                "memory": mem.get('content', ''),
                "hits": mem.get('hit_count', 0),
                "date": mem.get('created_at', '')[:10]
            })
            
        # LLM Prompt
        prompt = f"""‰Ω†ÊòØËÆ∞ÂøÜÈáçÊûÑÊû∂ÊûÑÂ∏à„ÄÇ

### ËæìÂÖ•Êï∞ÊçÆÔºàËøô‰∫õÊòØÁªèÂ∏∏Ë¢´ÂõûÂøÜËµ∑ÁöÑÈ´òÈ¢ëËÆ∞ÂøÜÔºåËØ¥ÊòéÂÆÉ‰ª¨ÂæàÈáçË¶ÅÔºâÔºö
{json.dumps(input_list, ensure_ascii=False, indent=2)}
Ê≥®ÊÑèÔºöÂØπËØùÊó•ÂøóÊòØÁî±ËØ≠Èü≥ËΩ¨ÂΩïÁîüÊàêÁöÑÔºåÂõ†Ê≠§ÂèØËÉΩÂ≠òÂú®ÈîôÂà´Â≠óÊàñË∞êÈü≥Â≠ó‰ª•ÂèäÊó†ÊÑè‰πâÁöÑÈîô‰π±ÊñáÂ≠óÔºåËØ∑ËøõË°å‰øÆÊ≠£„ÄÇ

### Â§ÑÁêÜÈÄªËæëÔºö
- ÊèêÁÇºÔºöËøô‰∫õËÆ∞ÂøÜË¢´ÂèçÂ§çÊèêÂèäÔºåËØ∑ÊèêÂèñÂÖ∂‰∏≠ÊúÄÊ†∏ÂøÉ„ÄÅÊúÄÊåÅ‰πÖÁöÑ‰ø°ÊÅØ„ÄÇ
- ÂçáÂçéÔºöÂ∞ÜÂÖ∑‰ΩìÁöÑ‰∫ã‰ª∂ËΩ¨Âåñ‰∏∫Ê∑±ÂàªÁêÜËß£ÔºàÂ¶ÇÊÄßÊ†ºÁâπË¥®„ÄÅÂÅèÂ•Ω„ÄÅÊΩúÂú®ÊÑèËØÜ
- ÂéªÈáçÔºöÂ¶ÇÊûúÂ§öÊù°ËÆ∞ÂøÜÈáçÂ§çÔºåËØ∑ÂêàÂπ∂‰∏∫‰∏ÄÊù°„ÄÇ
- ÁüõÁõæÔºö‰øÆÊ≠£ËøáÊó∂‰ø°ÊÅØ„ÄÇ
- Âè•ÂØπËØù‰∏≠ÂèØËÉΩÂåÖÂê´Â§ö‰∏™‰∏çÂêåÁöÑ‰∏ª‰ΩìÂíå‰∫ãÂÆûÔºåËØ∑ÊääÂÆÉÂàÜÁ¶ªÊàêÂ§ö‰∏™"memory"ÁâáÊÆµ

### ËæìÂá∫Ê†ºÂºè (‰ªÖ JSON ÂàóË°®):
[
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì1+‰∫ãÂÆû][Âü∫‰∫éÈ´òÈ¢ëÂõûÂøÜÁöÑÁÆÄÁü≠Ê∑±ÂàªÊ¥ûÂØü]"}},
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì1+‰∫ãÂÆû][Âü∫‰∫éÈ´òÈ¢ëÂõûÂøÜÁöÑÁÆÄÁü≠Ê∑±ÂàªÊ¥ûÂØü]"}},
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì2+‰∫ãÂÆû][Âü∫‰∫éÈ´òÈ¢ëÂõûÂøÜÁöÑÁÆÄÁü≠Ê∑±ÂàªÊ¥ûÂØü]"}},
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∏ª‰Ωì3+‰∫ãÂÆû][Âü∫‰∫éÈ´òÈ¢ëÂõûÂøÜÁöÑÁÆÄÁü≠Ê∑±ÂàªÊ¥ûÂØü]"}},
]
Ê≥®ÊÑèÔºöÂøÖÈ°ªÊòØJSONÊ†ºÂºèÂàóË°®„ÄÇ‰∏çË¶ÅËæìÂá∫ÂÖ∂‰ªñÂÜÖÂÆπ„ÄÇ
"""
        
        try:
            if not self.llm_client:
                logger.warning("[Dreaming] LLM client not available for consolidator")
                return

            # === DEBUG: ÊâìÂç∞ÂèëÈÄÅÁªô LLM ÁöÑÂÜÖÂÆπ ===
            logger.info(f"[Consolidator] üì§ Sending to LLM ({len(pending_mems)} memories):")
            logger.info(f"[Consolidator] Prompt:\n{prompt}")

            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a memory consolidator. Output JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5
            )
            
            content = response.choices[0].message.content.strip()

            # === DEBUG: ÊâìÂç∞ LLM ËøîÂõûÁöÑÂÜÖÂÆπ ===
            logger.info(f"[Consolidator] üì• LLM Response received")
            logger.info(f"[Consolidator] Raw response:\n{content}")
            
            # Clean markdown
            if content.startswith("```json"): 
                content = content.split("\n", 1)[1]
            if content.endswith("```"): 
                content = content.rsplit("\n", 1)[0]
                
            consolidated_memories = json.loads(content)
            
            # Ensure list
            if isinstance(consolidated_memories, dict):
                consolidated_memories = consolidated_memories.get("memories", [consolidated_memories])

            # Save consolidated memories
            for item in consolidated_memories:
                raw_text = item.get("memory", "")
                if not raw_text: continue
                
                # Embedding
                vector = [0.0] * 384
                if hasattr(self.memory, 'encoder') and self.memory.encoder:
                    try:
                        vector = self.memory.encoder(raw_text)
                    except Exception as e:
                        logger.warning(f"Failed to encode memory: {e}")
                
                await self.memory.add_episodic_memory(
                    character_id=self.character_id,
                    content=raw_text,
                    embedding=vector,
                    # Important: New consolidated memories start fresh
                    status="active",
                    hit_count=0 
                )
            
            # Archive OLD memories
            for old_mem in pending_mems:
                mem_id = old_mem.get('id', '')
                if mem_id:
                     await self.memory.db.query(f"UPDATE {mem_id} SET status = 'archived';")
                
            logger.info(f"[Dreaming] Consolidated {len(pending_mems)} high-hit memories -> {len(consolidated_memories)} new insights")
            
        except json.JSONDecodeError as e:
            logger.error(f"[Dreaming] Failed to parse consolidator response: {e}")
        except Exception as e:
            logger.error(f"[Dreaming] Consolidator failed for {self.character_id}: {e}")


    async def _run_consolidator_with_batch(self, batch):
        """
        ‰ΩøÁî® BatchManager ÊâπÊ¨°Â§ÑÁêÜ Consolidator
        
        Â§ÑÁêÜÊåáÂÆöÊâπÊ¨°‰∏≠ÁöÑËÆ∞ÂøÜÔºàÁî± search_hybrid Ê£ÄÁ¥¢ÂàõÂª∫Ôºâ
        Ëøô‰∫õËÆ∞ÂøÜËØ≠‰πâÁõ∏ÂÖ≥ÔºåÂ∫îËØ•‰∏ÄËµ∑Êï¥Âêà
        
        Args:
            batch: ConsolidationBatch ÂØπË±°
        """
        from consolidation_batch import ConsolidationBatch
        
        if not isinstance(batch, ConsolidationBatch):
            logger.error("[Dreaming] Invalid batch type")
            return
            
        memory_ids = batch.retrieved_ids
        if not memory_ids:
            logger.debug(f"[Dreaming] Batch {batch.batch_id} has no memories")
            return
        
        # Ê†πÊçÆ ID Êü•ËØ¢ËÆ∞ÂøÜÂÜÖÂÆπ
        pending_mems = []
        for mem_id in memory_ids:
            try:
                result = await self.memory.db.query(f"SELECT * FROM {mem_id}")
                if result and isinstance(result, list) and len(result) > 0:
                    mem = result[0]
                    if isinstance(mem, dict) and 'result' in mem:
                        pending_mems.extend(mem['result'])
                    elif isinstance(mem, dict):
                        pending_mems.append(mem)
            except Exception as e:
                logger.warning(f"Failed to fetch memory {mem_id}: {e}")
        
        if not pending_mems:
            logger.debug(f"[Dreaming] No valid memories in batch {batch.batch_id}")
            self.memory.batch_manager.complete_batch(batch.batch_id)
            return
        
        # ËÆ∞ÂΩïÂèëÈÄÅÁªô LLM ÁöÑ ID
        sent_ids = [str(m.get('id', '')) for m in pending_mems if m.get('id')]
        self.memory.batch_manager.mark_sent_to_llm(batch.batch_id, sent_ids)
        
        # ÂáÜÂ§á LLM ËæìÂÖ•
        input_list = []
        for i, mem in enumerate(pending_mems):
            input_list.append({
                "id": str(i + 1),
                "memory": mem.get('content', '')
            })
            
        # LLM Prompt
        prompt = f"""‰Ω†ÊòØËÆ∞ÂøÜÈáçÊûÑÊû∂ÊûÑÂ∏à„ÄÇ

### ËæìÂÖ•Êï∞ÊçÆÔºàËøô‰∫õÊòØËØ≠‰πâÁõ∏ÂÖ≥ÁöÑËÆ∞ÂøÜÔºåÊù•Ëá™Âêå‰∏ÄÊ¨°Ê£ÄÁ¥¢ÔºâÔºö
{json.dumps(input_list, ensure_ascii=False, indent=2)}
Ê≥®ÊÑèÔºöÂØπËØùÊó•ÂøóÊòØÁî±ËØ≠Èü≥ËΩ¨ÂΩïÁîüÊàêÁöÑÔºåÂõ†Ê≠§ÂèØËÉΩÂ≠òÂú®ÈîôÂà´Â≠óÊàñË∞êÈü≥Â≠ó‰ª•ÂèäÊó†ÊÑè‰πâÁöÑÈîô‰π±ÊñáÂ≠óÔºåËØ∑ËøõË°å‰øÆÊ≠£„ÄÇ

### Â§ÑÁêÜÈÄªËæëÔºö
- ÂêàÂπ∂ÔºöÂ∞ÜÊâÄÊúâÂèØ‰ª•ÂêàÂπ∂ÁöÑËÆ∞ÂøÜÂêàÂπ∂Êàê‰∏ÄÊù°„ÄÇ
- Ê∑±ÂàªÔºöÊèêÂèñÊ∑±Â±ÇÊ¥ûÂØüÔºåÂèçÊò†‰∏ª‰ΩìÁöÑÊÄßÊ†º„ÄÅÂÅèÂ•Ω„ÄÅÊΩúÂú®ÊÑèËØÜ„ÄÇ
- ÁüõÁõæÔºöÂ¶ÇÊúâÁüõÁõæÔºå‰øùÁïôÊúÄÊñ∞‰ø°ÊÅØ„ÄÇ
- ÊØèÊÆµÂØπËØù‰∏≠ÂèØËÉΩÂåÖÂê´Â§ö‰∏™‰∏çÂêåÁöÑ‰∫ãÂÆûÔºåËØ∑ÊääÂÆÉÂàÜÁ¶ªÊàêÂ§ö‰∏™"memory"

### ËæìÂá∫Ê†ºÂºè (‰ªÖ JSON ÂàóË°®):
[
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∫ãÂÆû] [ÁÆÄÁü≠ÁöÑÊ∑±ÂàªÊ¥ûÂØü]"}},
  {{"memory": "[Êó•Êúü+Êó∂Èó¥] [‰∫ãÂÆû] [ÁÆÄÁü≠ÁöÑÊ∑±ÂàªÊ¥ûÂØü]"}}
]
"""
        
        try:
            if not self.llm_client:
                logger.warning("[Dreaming] No LLM client, skipping batch consolidation")
                return
                
            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a memory consolidator. Output JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5
            )
            
            content = response.choices[0].message.content.strip()
            
            # Clean markdown
            if content.startswith("```json"): 
                content = content.split("\n", 1)[1]
            if content.endswith("```"): 
                content = content.rsplit("\n", 1)[0]
            
            consolidated = json.loads(content)
            if isinstance(consolidated, dict):
                consolidated = consolidated.get("memories", [consolidated])
            
            # 1. ÂΩíÊ°£ÊóßËÆ∞ÂøÜ
            for mem in pending_mems:
                mem_id = mem.get('id', '')
                if mem_id:
                    await self.memory.db.query(f"UPDATE {mem_id} SET status = 'archived';")
            
            # 2. ÊèíÂÖ•Êñ∞ËÆ∞ÂøÜ
            for item in consolidated:
                raw_text = item.get("memory", "")
                if not raw_text: continue
                
                vector = [0.0] * 384
                if hasattr(self.memory, 'encoder') and self.memory.encoder:
                    try:
                        vector = self.memory.encoder(raw_text)
                    except:
                        pass
                
                await self.memory.add_episodic_memory(
                    character_id=self.character_id,
                    content=raw_text,
                    embedding=vector,
                    status="active"
                )
            
            # 3. ÂÆåÊàêÊâπÊ¨°
            self.memory.batch_manager.complete_batch(batch.batch_id)
            
            logger.info(f"[Dreaming] üì¶ Batch {batch.batch_id}: {len(pending_mems)} -> {len(consolidated)} memories")
            
        except Exception as e:
            logger.error(f"[Dreaming] Batch consolidation failed: {e}")
            self.memory.batch_manager.fail_batch(batch.batch_id, str(e))


    # ==================== Phase 3: Soul Evolution ====================
    
    def _get_soul_manager(self):
        """Âª∂ËøüÂä†ËΩΩ SoulManagerÔºåÈÅøÂÖçÂæ™ÁéØÂØºÂÖ•"""
        if self._soul_manager is None:
            from soul_manager import SoulManager
            self._soul_manager = SoulManager(character_id=self.character_id)
        return self._soul_manager
    
    def accumulate_for_evolution(self, text: str, count: int = 1):
        """
        Á¥ØÁßØÂ§ÑÁêÜÁöÑÊñáÊú¨ÂíåËÆ∞ÂøÜÊï∞ÈáèÔºå‰æõ Soul Evolution ‰ΩøÁî®„ÄÇ
        Áî± Extractor Ë∞ÉÁî®„ÄÇ
        """
        self._accumulated_text_for_evolution += text + "\n"
        self._processed_memories_since_evolution += count
    
    async def _check_and_trigger_soul_evolution(self):
        """
        Ê£ÄÊü•Âπ∂Ëß¶Âèë Soul Evolution„ÄÇ
        
        Ëß¶ÂèëÊù°‰ª∂ÔºàÈúÄÂÖ®ÈÉ®Êª°Ë∂≥ÔºâÔºö
        1. Ë∑ùÁ¶ª‰∏äÊ¨°ÊºîÂåñ >= min_interval_minutes ÂàÜÈíü
        2. Á¥ØËÆ°Â§ÑÁêÜ >= min_memories_threshold Êù°ËÆ∞ÂøÜ
        3. Á¥ØËÆ°ÊñáÊú¨ÈïøÂ∫¶ >= min_text_length Â≠óÁ¨¶
        """
        config = self.soul_evolution_config
        
        # Êù°‰ª∂1: Êó∂Èó¥Èó¥ÈöîÊ£ÄÊü•
        if self._last_soul_evolution_time:
            elapsed = (datetime.now() - self._last_soul_evolution_time).total_seconds() / 60
            if elapsed < config["min_interval_minutes"]:
                logger.debug(f"[Soul Evolution] Skipped: Only {elapsed:.1f}/{config['min_interval_minutes']} minutes since last evolution")
                return
        
        # Êù°‰ª∂2: ËÆ∞ÂøÜÊï∞ÈáèÊ£ÄÊü•
        if self._processed_memories_since_evolution < config["min_memories_threshold"]:
            logger.debug(f"[Soul Evolution] Skipped: Only {self._processed_memories_since_evolution}/{config['min_memories_threshold']} memories processed")
            return
        
        # Êù°‰ª∂3: ÊñáÊú¨ÈïøÂ∫¶Ê£ÄÊü•
        if len(self._accumulated_text_for_evolution) < config["min_text_length"]:
            logger.debug(f"[Soul Evolution] Skipped: Only {len(self._accumulated_text_for_evolution)}/{config['min_text_length']} chars accumulated")
            return
        
        # ÊâÄÊúâÊù°‰ª∂Êª°Ë∂≥ÔºåËß¶ÂèëÊºîÂåñ
        logger.info(f"[Soul Evolution] üå± All conditions met! Triggering evolution...")
        await self._analyze_soul_evolution(self._accumulated_text_for_evolution)
        
        # ÈáçÁΩÆËÆ°Êï∞Âô®
        self._last_soul_evolution_time = datetime.now()
        self._processed_memories_since_evolution = 0
        self._accumulated_text_for_evolution = ""
    
    async def _analyze_soul_evolution(self, text_batch: str):
        """
        ÂàÜÊûêÊúÄËøëÁöÑËÆ∞ÂøÜÔºåÊºîÂåñ Big Five„ÄÅPAD„ÄÅTraits Âíå Mood„ÄÇ
        ‰ΩøÁî® LLM JSON Output Ê®°Âºè„ÄÇ
        """
        if not self.llm_client:
            logger.warning("[Soul Evolution] No LLM client available")
            return
        
        soul = self._get_soul_manager()
        
        # 1. Ëé∑ÂèñÈöèÊú∫ËÆ∞ÂøÜ‰Ωú‰∏∫‰∏ä‰∏ãÊñá
        random_memories = []
        try:
            query = """
            SELECT content FROM episodic_memory 
            WHERE character_id = $character_id AND status = 'active'
            ORDER BY RAND() LIMIT 10;
            """
            result = await self.memory.db.query(query, {"character_id": self.character_id})
            if result and isinstance(result, list):
                first = result[0]
                if isinstance(first, dict) and 'result' in first:
                    random_memories = [m.get('content', '') for m in (first['result'] or [])]
                elif isinstance(first, dict) and 'content' in first:
                    random_memories = [m.get('content', '') for m in result]
        except Exception as e:
            logger.warning(f"[Soul Evolution] Failed to fetch random memories: {e}")
        
        random_mem_text = "\n".join([f"- {m}" for m in random_memories[:10]])
        
        # 2. Ëé∑ÂèñÂΩìÂâçÁä∂ÊÄÅ
        current_traits = soul.profile.get("personality", {}).get("traits", [])
        current_big_five = soul.profile.get("personality", {}).get("big_five", {})
        current_pad = soul.profile.get("personality", {}).get("pad_model", {})
        current_mood = soul.profile.get("state", {}).get("current_mood", "neutral")
        
        # 3. ÊûÑÂª∫ Prompt
        system_prompt = """You are a master-level psychology expert. Your goal is to evolve the internal state of a character based on their recent experiences and past memories.

You must output a valid JSON object strictly following the structure below.

Your Task:
Analyze the Recent Interactions in the context of the character's history.
Determine how the character's internal state should shift.
Output the NEW ABSOLUTE VALUES for Big Five and PAD, and a potentially updated list of Traits.
Also select the most appropriate "current_mood" tag from the allowed list: 
[happy], [sad], [angry], [neutral], [tired], [excited], [shy], [obsessed], [confused]

EXAMPLE JSON OUTPUT:
{
    "new_traits": ["<derive 4-5 traits from interaction>"],
    "new_big_five": {
        "openness": <choose number between 0.0 and 1.0>,
        "conscientiousness": <choose number between 0.0 and 1.0>,
        "extraversion": <choose number between 0.0 and 1.0>,
        "agreeableness": <choose number between 0.0 and 1.0>,
        "neuroticism": <choose number between 0.0 and 1.0>
    },
    "new_pad": {
        "pleasure": <choose number between 0.0 and 1.0>,
        "arousal": <choose number between 0.0 and 1.0>,
        "dominance": <choose number between 0.0 and 1.0>
    },
    "current_mood": "(choose from: [happy], [sad], [angry], [neutral], [tired], [excited], [shy], [obsessed], [confused])"
}
"""

        user_prompt = f"""Current State:
- Traits: {current_traits}
- Big Five: {current_big_five}
- PAD Model: {current_pad}
- Current Mood: {current_mood}

Random Past Memories (Context):
{random_mem_text}

Recent Interactions (Focus on this):
"{text_batch[:2000]}"

Instruction:
Based on the interactions, output the NEW state. 
- **Big Five and PAD values must be specific floats between 0.0 and 1.0.**
- **Do NOT simply copy the Current State.** You must decide if the recent interaction implies a change (increase or decrease).
- If the interaction is neutral, small changes are fine. If emotional, larger shifts are expected.
- Determine if 'Traits' need to change (keep 4-5 adjectives).
- Select a 'current_mood' from the allowed list.
- **You MUST return ALL fields (new_big_five, new_pad, current_mood) in the JSON.**
- Return valid JSON only.
"""
        
        try:
            logger.info(f"[Soul Evolution] üß† Calling LLM for Soul Evolution (JSON Mode)...")
            
            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.4,  # Slightly higher temp to encourage change
                response_format={"type": "json_object"} if hasattr(self.llm_client, 'response_format') else None
            )
            
            content = response.choices[0].message.content.strip()
            logger.info(f"[Soul Evolution] Raw Response: {content[:200]}...")
            
            # Ê∏ÖÁêÜ JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].strip()
            
            data = json.loads(content)
            
            # Êõ¥Êñ∞ Soul
            new_traits = data.get("new_traits")
            new_big_five = data.get("new_big_five")
            new_pad = data.get("new_pad")
            new_mood = data.get("current_mood")
            
            if new_traits and isinstance(new_traits, list):
                soul.update_traits(new_traits)
            
            if new_big_five and isinstance(new_big_five, dict):
                soul.update_big_five(new_big_five)
                
            if new_pad and isinstance(new_pad, dict):
                soul.update_pad(new_pad)
            
            if new_mood:
                soul.update_current_mood(new_mood)
            
            logger.info(f"[Soul Evolution] ‚ú® Evolution complete! Traits: {new_traits}, Mood: {new_mood}")
                
        except json.JSONDecodeError as e:
            logger.error(f"[Soul Evolution] Failed to parse LLM response as JSON: {e}")
        except Exception as e:
            logger.error(f"[Soul Evolution] Evolution analysis failed: {e}")
            import traceback
            traceback.print_exc()


# Test Stub
if __name__ == "__main__":
    import asyncio
    from surreal_memory import SurrealMemory
    
    async def main():
        mem = SurrealMemory(character_id="lillian")
        await mem.connect()
        
        # Create dreaming instance for specific character
        dream = Dreaming(memory_client=mem, character_id="lillian")
        await dream.process_memories()
        
        await mem.db.close()
        
    asyncio.run(main())
