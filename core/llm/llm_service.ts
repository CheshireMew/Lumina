import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, SystemMessage, AIMessage, BaseMessage } from "@langchain/core/messages";
import { Message } from './types';

export class LLMService {
    private chatModel: ChatOpenAI | null = null;
    private systemPrompt: string = ""; // Set dynamically by App.tsx based on active character

    constructor(apiKey?: string, baseUrl?: string, modelName?: string) {
        if (apiKey) {
            this.init(apiKey, baseUrl, modelName);
        }
    }

    public init(apiKey: string, baseUrl: string = 'https://api.deepseek.com/v1', modelName: string = 'deepseek-chat') {
        console.log(`Initializing LLM Service with BaseURL: ${baseUrl}, Model: ${modelName}, KeyLength: ${apiKey?.length}`);
        this.chatModel = new ChatOpenAI({
            apiKey: apiKey, // Explicitly pass as apiKey
            openAIApiKey: apiKey, // Backwards usage
            configuration: {
                baseURL: baseUrl,
            },
            modelName: modelName,
            temperature: 0.7,
        });
    }

    public async chat(message: string): Promise<string> {
        if (!this.chatModel) {
            console.warn('LLM Service not initialized, returning mock response.');
            return "Please configure your API Key in settings first! (LLM not initialized)";
        }

        try {
            const response = await this.chatModel.invoke([
                new SystemMessage(this.systemPrompt),
                new HumanMessage(message),
            ]);

            return response.content as string;
        } catch (error) {
            console.error("LLM Chat Error:", error);
            return `Error: ${error instanceof Error ? error.message : String(error)}`;
        }
    }

    public setSystemPrompt(prompt: string) {
        this.systemPrompt = prompt;
    }

    /**
     * æµå¼èŠå¤©ï¼šé€ token è¿”å› AI å›å¤
     * @param message ç”¨æˆ·æ¶ˆæ¯
     * @param onToken æ¯æ”¶åˆ°ä¸€ä¸ª token æ—¶çš„å›è°ƒ
     * @returns Promise<string> å®Œæ•´å›å¤
     */
    public async chatStream(
        message: string,
        onToken: (token: string) => void
    ): Promise<string> {
        if (!this.chatModel) {
            console.warn('LLM Service not initialized');
            const errorMsg = "Please configure your API Key in settings first!";
            onToken(errorMsg);
            return errorMsg;
        }

        try {
            const stream = await this.chatModel.stream([
                new SystemMessage(this.systemPrompt),
                new HumanMessage(message),
            ]);

            let fullResponse = '';
            for await (const chunk of stream) {
                const content = chunk.content as string;
                if (content) {
                    fullResponse += content;
                    onToken(content); // å®æ—¶å›è°ƒ
                }
            }

            return fullResponse;
        } catch (error) {
            console.error("LLM Stream Error:", error);
            const errorMsg = `Error: ${error instanceof Error ? error.message : String(error)}`;
            onToken(errorMsg);
            return errorMsg;
        }
    }

    /**
     * å¸¦å†å²è®°å½•çš„æµå¼èŠå¤©
     * @param conversationHistory å®Œæ•´å¯¹è¯å†å²
     * @param userMessage ç”¨æˆ·å½“å‰æ¶ˆæ¯
     * @param contextWindow ä¿ç•™è½®æ•°
     * @param onToken Token å›è°ƒ
     * @param summary å¯é€‰çš„å†å²æ‘˜è¦
     * @returns Promise<string> å®Œæ•´å›å¤
     */
    public async chatStreamWithHistory(
        conversationHistory: Message[],
        userMessage: string,
        contextWindow: number,
        onToken: (token: string) => void,
        summary?: string,
        longTermMemory?: string,
        userName: string = 'User',
        charName: string = 'Assistant'
    ): Promise<string> {
        if (!this.chatModel) {
            throw new Error("Chat model not initialized");
        }

        try {
            // âœ… DeepSeek ç¼“å­˜ä¼˜åŒ–ç»“æ„ï¼š
            // 1. å†å²å¯¹è¯ï¼ˆå¯ç¼“å­˜å‰ç¼€ï¼‰
            // 2. å½“å‰ç”¨æˆ·æ¶ˆæ¯
            // 3. System Prompt + è®°å¿† + æ‘˜è¦ï¼ˆåˆå¹¶ä¸ºä¸€æ¡ SystemMessageï¼‰
            
            const messages: BaseMessage[] = [];

            // 1ï¸âƒ£ å†å²å¯¹è¯ä½œä¸ºå¯ç¼“å­˜å‰ç¼€ï¼ˆæœ€å‰é¢ï¼Œæœ€ç¨³å®šï¼‰
            // åº”ç”¨æ»‘åŠ¨çª—å£ï¼šåªä¿ç•™æœ€è¿‘çš„ contextWindow è½®å¯¹è¯
            const maxHistoryMessages = contextWindow * 2;
            const recentHistory = conversationHistory.slice(-maxHistoryMessages);

            // å°†å†å²è½¬æ¢ä¸º LangChain æ¶ˆæ¯ï¼ˆä½¿ç”¨çœŸå®ç”¨æˆ·åå’Œè§’è‰²åï¼Œé¿å…å‡ºæˆï¼‰
            for (const msg of recentHistory) {
                if (msg.role === 'user') {
                    messages.push(new HumanMessage({ content: msg.content, name: userName }));
                } else if (msg.role === 'assistant') {
                    messages.push(new AIMessage({ content: msg.content, name: charName }));
                }
            }

            // 2ï¸âƒ£ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆçº¯æ¶ˆæ¯ï¼Œä¸é™„åŠ ä¸Šä¸‹æ–‡ï¼‰
            messages.push(new HumanMessage({ content: userMessage, name: userName }));

            // 3ï¸âƒ£ åŠ¨æ€ System Promptï¼ˆæ”¾æœ€åï¼ŒåŒ…å«æ‰€æœ‰åŠ¨æ€ä¸Šä¸‹æ–‡ï¼‰
            let dynamicSystemPrompt = this.systemPrompt;

            // é™„åŠ é•¿æœŸè®°å¿†
            if (longTermMemory) {
                dynamicSystemPrompt += `\n\n## ç›¸å…³è®°å¿†ï¼ˆæ¥è‡ªè¿‡å¾€å¯¹è¯ï¼‰\n${longTermMemory}\n\nè¯·åˆ©ç”¨è¿™äº›è®°å¿†æä¾›ä¸ªæ€§åŒ–çš„å›å¤ï¼Œä½†ä¸è¦æ˜ç¡®æåŠä½ åœ¨é˜…è¯»è®°å¿†ï¼Œé™¤éç›¸å…³ã€‚`;
            }

            // é™„åŠ å¯¹è¯æ‘˜è¦
            if (summary) {
                dynamicSystemPrompt += `\n\n## ä¹‹å‰çš„å¯¹è¯æ‘˜è¦\n${summary}`;
            }

            messages.push(new SystemMessage(dynamicSystemPrompt));

            // ========== [DEBUG] è¯¦ç»†çš„è¯·æ±‚å†…å®¹æ‰“å° ==========
            console.log('\n\n' + 'â•'.repeat(80));
            console.log('ğŸ“¤ å‘é€ç»™ DeepSeek çš„å®Œæ•´è¯·æ±‚å†…å®¹');
            console.log('â•'.repeat(80));
            
            console.log(`\nğŸ“‹ è¯·æ±‚é…ç½®:`);
            console.log(`   - ç”¨æˆ·å: "${userName}"`);
            console.log(`   - è§’è‰²å: "${charName}"`);
            console.log(`   - æ¶ˆæ¯æ€»æ•°: ${messages.length}`);
            console.log(`   - Context Window: ${contextWindow} è½®`);
            console.log(`   - å†å²å¯¹è¯: ${recentHistory.length} æ¡`);
            
            console.log(`\nğŸ“¨ æ¶ˆæ¯ç»“æ„è¯¦æƒ…:\n`);
            
            messages.forEach((msg, index) => {
                let roleIcon = '';
                let roleText = '';
                let msgName = '';
                
                if (msg._getType() === 'human') {
                    roleIcon = 'ğŸ‘¤';
                    roleText = 'User';
                    msgName = (msg as any).name || 'Unknown';
                } else if (msg._getType() === 'ai') {
                    roleIcon = 'ğŸ¤–';
                    roleText = 'Assistant';
                    msgName = (msg as any).name || 'Unknown';
                } else if (msg._getType() === 'system') {
                    roleIcon = 'âš™ï¸';
                    roleText = 'System';
                    msgName = 'System';
                }
                
                const content = msg.content.toString();
                const preview = content.substring(0, 100);
                
                console.log(`[${index + 1}] ${roleIcon} ${roleText} (name: "${msgName}")`);
                console.log(`    Preview: ${preview}${content.length > 100 ? '...' : ''}`);
                console.log(`    Length: ${content.length} chars\n`);
            });
            
            // æ‰“å° API æ ¼å¼
            const apiMessages = messages.map(msg => {
                let role = '';
                if (msg._getType() === 'human') role = 'user';
                else if (msg._getType() === 'ai') role = 'assistant';
                else if (msg._getType() === 'system') role = 'system';
                
                const apiMsg: any = {
                    role,
                    content: msg.content.toString()
                };
                
                const msgName = (msg as any).name;
                if (msgName) apiMsg.name = msgName;
                
                return apiMsg;
            });
            
            console.log('â•'.repeat(80));
            console.log('ğŸ“¡ å®é™… API è¯·æ±‚æ ¼å¼ (JSON):');
            console.log('â•'.repeat(80));
            console.log(JSON.stringify({
                model: 'deepseek-chat',
                messages: apiMessages,
                stream: true,
                temperature: 0.7
            }, null, 2));
            
            console.log('\n' + 'â•'.repeat(80));
            console.log('ğŸ’¾ å®Œæ•´ System Prompt å†…å®¹:');
            console.log('â•'.repeat(80));
            console.log(dynamicSystemPrompt);
            
            console.log('\n' + 'â•'.repeat(80));
            console.log('ğŸ” ç¼“å­˜åˆ†æ:');
            console.log('â•'.repeat(80));
            
            const historyTokenEstimate = recentHistory.reduce((sum, msg) => 
                sum + Math.ceil(msg.content.length / 4), 0
            );
            const currentTokenEstimate = Math.ceil(userMessage.length / 4);
            const systemTokenEstimate = Math.ceil(dynamicSystemPrompt.length / 4);
            const totalTokens = historyTokenEstimate + currentTokenEstimate + systemTokenEstimate;
            
            console.log(`\n1ï¸âƒ£ å†å²å¯¹è¯ (å¯ç¼“å­˜): ~${historyTokenEstimate} tokens`);
            console.log(`2ï¸âƒ£ å½“å‰æ¶ˆæ¯: ~${currentTokenEstimate} tokens`);
            console.log(`3ï¸âƒ£ System Prompt: ~${systemTokenEstimate} tokens`);
            console.log(`\n   æ€»è®¡: ~${totalTokens} tokens`);
            console.log(`   å¯ç¼“å­˜æ¯”ä¾‹: ${((historyTokenEstimate / totalTokens) * 100).toFixed(1)}%`);
            console.log(`   ğŸ’° é¢„ä¼°èŠ‚çœ: 40-60% (ç¬¬2è½®èµ·)\n`);
            
            console.log('â•'.repeat(80) + '\n');
            // ========== [DEBUG END] ==========

            console.log(`[LLMService] Sending ${messages.length} messages (context window: ${contextWindow} turns)`);

            // 4ï¸âƒ£ æµå¼è¯·æ±‚
            const stream = await this.chatModel.stream(messages);

            let fullResponse = '';
            for await (const chunk of stream) {
                const content = chunk.content as string;
                if (content) {
                    fullResponse += content;
                    onToken(content);
                }
            }

            // [DEBUG] Log Response content
            console.log(`\n--- [LLM Output from ${charName}] ---`);
            console.log(fullResponse.substring(0, 500) + (fullResponse.length > 500 ? '...' : ''));
            console.log('--------------------\n');

            return fullResponse;
        } catch (error) {
            console.error("LLM Stream With History Error:", error);
            const errorMsg = `Error: ${error instanceof Error ? error.message : String(error)}`;
            onToken(errorMsg);
            return errorMsg;
        }
    }

    /**
     * ç”Ÿæˆå¯¹è¯æ‘˜è¦
     * @param messages è¦æ‘˜è¦çš„å¯¹è¯å†å²
     * @returns Promise<string> æ‘˜è¦å†…å®¹
     */
    public async generateSummary(messages: Message[]): Promise<string> {
        if (!this.chatModel || messages.length === 0) {
            return '';
        }

        try {
            // æ„å»ºæ‘˜è¦æç¤º
            let conversationText = '';
            for (const msg of messages) {
                const roleLabel = msg.role === 'user' ? 'ç”¨æˆ·' : 'Lumina';
                conversationText += `${roleLabel}ï¼š${msg.content}\n`;
            }

            const summaryPrompt = `è¯·å°†ä»¥ä¸‹å¯¹è¯æ€»ç»“ä¸ºä¸€æ®µç®€çŸ­çš„æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼š\n\n${conversationText}\n\næ‘˜è¦ï¼š`;

            const response = await this.chatModel.invoke([
                new SystemMessage("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç®€æ´å‡†ç¡®åœ°æ¦‚æ‹¬å¯¹è¯å†…å®¹ã€‚"),
                new HumanMessage(summaryPrompt)
            ]);

            return response.content as string;
        } catch (error) {
            console.error("Summary Generation Error:", error);
            return '';
        }
    }

    /**
     * æ›´æ–°æ‘˜è¦ï¼šåˆå¹¶æ—§æ‘˜è¦å’Œæ–°å¯¹è¯
     * @param currentSummary å½“å‰æ‘˜è¦
     * @param newMessages éœ€è¦åˆå¹¶çš„æ–°å¯¹è¯
     * @returns Promise<string> æ›´æ–°åçš„æ‘˜è¦
     */
    public async updateSummary(currentSummary: string, newMessages: Message[]): Promise<string> {
        if (!this.chatModel || newMessages.length === 0) {
            return currentSummary;
        }

        try {
            let conversationText = '';
            for (const msg of newMessages) {
                const roleLabel = msg.role === 'user' ? 'ç”¨æˆ·' : 'Lumina';
                conversationText += `${roleLabel}ï¼š${msg.content}\n`;
            }

            const summaryPrompt = `è¿™æ˜¯ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ï¼š
"${currentSummary}"

è¿™æ˜¯éšåå‘ç”Ÿçš„å¯¹è¯ï¼š
${conversationText}

è¯·æ›´æ–°æ‘˜è¦ï¼ŒåŒ…å«ä¹‹å‰çš„å…³é”®ä¿¡æ¯å’Œæ–°çš„å¯¹è¯å†…å®¹ï¼Œä¿æŒåœ¨150å­—ä»¥å†…ï¼š`;

            const response = await this.chatModel.invoke([
                new SystemMessage("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ã€‚"),
                new HumanMessage(summaryPrompt)
            ]);

            return response.content as string;
        } catch (error) {
            console.error("Summary Update Error:", error);
            return currentSummary;
        }
    }
}

export const llmService = new LLMService();
