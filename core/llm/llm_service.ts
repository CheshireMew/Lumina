import { ChatOpenAI } from "@langchain/openai";
import {
  HumanMessage,
  SystemMessage,
  AIMessage,
  BaseMessage,
} from "@langchain/core/messages";
import { Message } from "./types";

export class LLMService {
  private chatModel: ChatOpenAI | null = null;
  private systemPrompt: string = "";
  private _apiKey: string = "";
  private _baseUrl: string = "";
  private _modelName: string = "";
  private _temperature: number = 0.7;
  private _topP: number = 1.0;
  private _presencePenalty: number = 0.0;
  private _frequencyPenalty: number = 0.0;

  constructor(
    apiKey?: string,
    baseUrl?: string,
    modelName?: string,
    temperature: number = 0.7,
    topP: number = 1.0,
    presencePenalty: number = 0.0,
    frequencyPenalty: number = 0.0
  ) {
    if (apiKey) {
      this.init(
        apiKey,
        baseUrl,
        modelName,
        temperature,
        topP,
        presencePenalty,
        frequencyPenalty
      );
    }
  }

  public init(
    apiKey: string,
    baseUrl: string = "https://api.deepseek.com/v1",
    modelName: string = "deepseek-chat",
    temperature: number = 0.7,
    topP: number = 1.0,
    presencePenalty: number = 0.0,
    frequencyPenalty: number = 0.0
  ) {
    // âš¡ Force IPv4 for local backend to avoid Node.js resolving 'localhost' to '::1'
    if (baseUrl.includes("localhost")) {
      baseUrl = baseUrl.replace("localhost", "127.0.0.1");
      console.log(`[LLMService] Sanitized BaseURL to IPv4: ${baseUrl}`);
    }

    this._apiKey = apiKey;
    this._baseUrl = baseUrl;
    this._modelName = modelName;
    this._temperature = temperature;
    this._topP = topP;
    this._presencePenalty = presencePenalty;
    this._frequencyPenalty = frequencyPenalty;

    console.log(
      `Initializing LLM Service with BaseURL: ${baseUrl}, Model: ${modelName}, Temp: ${temperature}, KeyLength: ${apiKey?.length}`
    );
    this.chatModel = new ChatOpenAI({
      apiKey: apiKey, // Explicitly pass as apiKey
      openAIApiKey: apiKey, // Backwards usage
      configuration: {
        baseURL: baseUrl,
      },
      modelName: modelName,
      temperature: temperature,
      topP: topP,
      presencePenalty: presencePenalty,
      frequencyPenalty: frequencyPenalty,
    });
  }

  public async chat(message: string): Promise<string> {
    if (!this.chatModel) {
      console.warn("LLM Service not initialized, returning mock response.");
      return "Please configure your API Key in settings first! (LLM not initialized)";
    }

    try {
      const response = await this.chatModel.invoke([
        new SystemMessage(this.systemPrompt),
        new HumanMessage(message),
      ]);

      return response.content as string;
    } catch (error) {
      console.error("LLM Chat Error:", error);
      return `Error: ${error instanceof Error ? error.message : String(error)}`;
    }
  }

  public setSystemPrompt(prompt: string) {
    this.systemPrompt = prompt;
  }

  /**
   * æµå¼èŠå¤©ï¼šé€ token è¿”å› AI å›å¤
   * @param message ç”¨æˆ·æ¶ˆæ¯
   * @param onToken æ¯æ”¶åˆ°ä¸€ä¸ª token æ—¶çš„å›è°ƒ
   * @returns Promise<string> å®Œæ•´å›å¤
   */
  public async chatStream(
    message: string,
    onToken: (token: string) => void
  ): Promise<string> {
    if (!this.chatModel) {
      console.warn("LLM Service not initialized");
      const errorMsg = "Please configure your API Key in settings first!";
      onToken(errorMsg);
      return errorMsg;
    }

    try {
      const stream = await this.chatModel.stream([
        new SystemMessage(this.systemPrompt),
        new HumanMessage(message),
      ]);

      let fullResponse = "";
      for await (const chunk of stream) {
        const content = chunk.content as string;
        if (content) {
          fullResponse += content;
          onToken(content); // å®æ—¶å›è°ƒ
        }
      }

      return fullResponse;
    } catch (error) {
      console.error("LLM Stream Error:", error);
      const errorMsg = `Error: ${
        error instanceof Error ? error.message : String(error)
      }`;
      onToken(errorMsg);
      return errorMsg;
    }
  }

  /**
   * å¸¦å†å²è®°å½•çš„æµå¼èŠå¤©
   * @param conversationHistory å®Œæ•´å¯¹è¯å†å²
   * @param userMessage ç”¨æˆ·å½“å‰æ¶ˆæ¯
   * @param contextWindow ä¿ç•™è½®æ•°
   * @param onToken Token å›è°ƒ
   * @param summary å¯é€‰çš„å†å²æ‘˜è¦
   * @returns Promise<string> å®Œæ•´å›å¤
   */
  /**
   * å¸¦å†å²è®°å½•çš„æµå¼èŠå¤©
   * @param conversationHistory å®Œæ•´å¯¹è¯å†å²
   * @param userMessage ç”¨æˆ·å½“å‰æ¶ˆæ¯
   * @param contextWindow ä¿ç•™è½®æ•°
   * @param onToken Token å›è°ƒ
   * @param summary å¯é€‰çš„å†å²æ‘˜è¦
   * @returns Promise<string> å®Œæ•´å›å¤
   */
  public async chatStreamWithHistory(
    conversationHistory: Message[],
    userMessage: string,
    contextWindow: number,
    onToken: (token: string, type?: "content" | "reasoning") => void,
    summary?: string,
    longTermMemory?: string,
    userName: string = "User",
    charName: string = "Assistant",
    role: "user" | "system" = "user",
    dynamicInstruction?: string,
    enableThinking: boolean = false, // âœ… NEW: Thinking Mode Toggle
    temperature?: number,
    topP?: number,
    presencePenalty?: number,
    frequencyPenalty?: number
  ): Promise<string> {
    if (!this.chatModel) {
      throw new Error("Chat model not initialized");
    }

    try {
      // 1ï¸âƒ£ Construct Messages using LangChain Types (Universal Format)
      const messages: BaseMessage[] = [];

      // A. Static System Prompt (Top Priority)
      if (this.systemPrompt) {
        messages.push(new SystemMessage(this.systemPrompt));
      }

      // B. Conversation History
      const maxHistoryMessages = contextWindow * 2;
      const recentHistory = conversationHistory.slice(-maxHistoryMessages);

      for (const msg of recentHistory) {
        if (msg.role === "user") {
          messages.push(
            new HumanMessage({ content: msg.content, name: userName })
          );
        } else if (msg.role === "assistant") {
          messages.push(
            new AIMessage({ content: msg.content, name: charName })
          );
        }
      }

      // C. Dynamic Context (Memory + Summary + Dynamic Instruction)
      // Placed right before the current message
      let dynamicContext = "";
      if (longTermMemory && longTermMemory.trim().length > 0) {
        dynamicContext += `\n\n## ç›¸å…³è®°å¿†\n${longTermMemory}`;
      }
      if (summary) {
        dynamicContext += `\n\n## ä¹‹å‰çš„å¯¹è¯æ‘˜è¦\n${summary}`;
      }
      if (dynamicInstruction) {
        dynamicContext += `\n\n${dynamicInstruction}`;
      }

      if (dynamicContext.trim().length > 0) {
        // Add as a separate System Message
        messages.push(new SystemMessage(dynamicContext.trim()));
      }

      // D. Current User Message
      if (role === "system") {
        messages.push(new SystemMessage(userMessage));
      } else {
        messages.push(
          new HumanMessage({ content: userMessage, name: userName })
        );
      }

      // ========== [DEBUG] Detailed Request Logging (Restored) ==========
      console.log("\n\n" + "â•".repeat(80));
      console.log("ğŸ“¤ [LLMService] Full Request Context");
      console.log("â•".repeat(80));

      console.log(`\nğŸ“‹ Configuration:`);
      console.log(`   - User: "${userName}"`);
      console.log(`   - Bot: "${charName}"`);
      console.log(
        `   - Model: "${
          enableThinking ? "deepseek-reasoner" : this._modelName || "default"
        }"`
      );
      console.log(`   - Context Window: ${contextWindow} turns`);
      console.log(`   - History Count: ${recentHistory.length} msgs`);

      console.log(`\nğŸ“¨ Message Structure:\n`);

      messages.forEach((msg, index) => {
        let roleIcon = "";
        let roleText = "";
        let msgName = "";

        if (msg._getType() === "human") {
          roleIcon = "ğŸ‘¤";
          roleText = "User";
          msgName = (msg as any).name || "Unknown";
        } else if (msg._getType() === "ai") {
          roleIcon = "ğŸ¤–";
          roleText = "Assistant";
          msgName = (msg as any).name || "Unknown";
        } else if (msg._getType() === "system") {
          roleIcon = "âš™ï¸";
          roleText = "System";
          msgName = "System";
        }

        const content = msg.content.toString();
        const preview = content.substring(0, 100).replace(/\n/g, " ");

        console.log(`[${index}] ${roleIcon} ${roleText} (name: "${msgName}")`);
        console.log(
          `    Preview: ${preview}${content.length > 100 ? "..." : ""}`
        );
        console.log(`    Length: ${content.length} chars\n`);
      });

      console.log("â•".repeat(80));
      console.log("ğŸ“¡ API Request Preview (JSON):");
      console.log("â•".repeat(80));

      const apiMessages = messages.map((msg) => {
        let role = "";
        if (msg._getType() === "human") role = "user";
        else if (msg._getType() === "ai") role = "assistant";
        else if (msg._getType() === "system") role = "system";
        return {
          role,
          content: msg.content.toString(),
          name: (msg as any).name,
        };
      });

      console.log(
        JSON.stringify(
          {
            model: enableThinking
              ? "deepseek-reasoner"
              : this._modelName || "unknown",
            messages: apiMessages,
            stream: true,
          },
          null,
          2
        )
      );

      console.log("\n" + "â•".repeat(80));
      console.log("ğŸ” Token Estimation (approx):");
      console.log("â•".repeat(80));

      const historyTokenEstimate = recentHistory.reduce(
        (sum, msg) => sum + Math.ceil(msg.content.length / 3.5),
        0
      );
      const currentTokenEstimate = Math.ceil(userMessage.length / 3.5);
      // Estimate system tokens from all system messages
      const systemMsgs = messages.filter((m) => m._getType() === "system");
      const systemTokenEstimate = systemMsgs.reduce(
        (sum, m) => sum + Math.ceil(m.content.toString().length / 3.5),
        0
      );
      const totalTokens =
        historyTokenEstimate + currentTokenEstimate + systemTokenEstimate;

      console.log(`1ï¸âƒ£ History: ~${historyTokenEstimate} tokens`);
      console.log(`2ï¸âƒ£ Current: ~${currentTokenEstimate} tokens`);
      console.log(`3ï¸âƒ£ System: ~${systemTokenEstimate} tokens`);
      console.log(`\n   Total: ~${totalTokens} tokens`);

      console.log("â•".repeat(80));

      let fullResponse = "";

      // ğŸ”„ BRANCH: Thinking Mode vs Standard Mode
      if (enableThinking) {
        // === Path A: Direct Fetch for DeepSeek Thinking ===
        const apiKey = this._apiKey;
        const baseUrl = this._baseUrl;
        const modelName = "deepseek-reasoner";

        // Convert LangChain messages to Raw API format
        const apiMessages = messages.map((m) => {
          const role =
            m._getType() === "human"
              ? "user"
              : m._getType() === "ai"
              ? "assistant"
              : "system";
          return { role, content: m.content.toString(), name: (m as any).name };
        });

        console.log(`[LLMService] ğŸ§  Thinking Mode Active (${modelName})`);

        const response = await fetch(`${baseUrl}/chat/completions`, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${apiKey}`,
          },
          body: JSON.stringify({
            model: modelName,
            messages: apiMessages,
            stream: true,
            temperature: temperature ?? this._temperature,
            top_p: topP ?? this._topP,
            presence_penalty: presencePenalty ?? this._presencePenalty,
            frequency_penalty: frequencyPenalty ?? this._frequencyPenalty,
          }),
        });

        if (!response.ok) {
          const err = await response.text();
          throw new Error(`DeepSeek API Error: ${response.status} ${err}`);
        }
        if (!response.body) throw new Error("No response body");

        const reader = response.body.getReader();
        const decoder = new TextDecoder("utf-8");

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          const chunk = decoder.decode(value, { stream: true });
          const lines = chunk.split("\n");
          for (const line of lines) {
            if (!line.startsWith("data: ") || line.trim() === "data: [DONE]")
              continue;
            try {
              const data = JSON.parse(line.slice(6));
              const delta = data.choices[0]?.delta;
              if (delta) {
                if (delta.reasoning_content) {
                  onToken(delta.reasoning_content, "reasoning");
                }
                if (delta.content) {
                  fullResponse += delta.content;
                  onToken(delta.content, "content");
                }
              }
            } catch (e) {}
          }
        }
      } else {
        // === Path B: Standard LangChain Stream ===
        console.log(
          `[LLMService] ğŸ’¬ Standard Mode (${this._modelName || "default"})`
        );
        const stream = await this.chatModel.stream(messages);

        for await (const chunk of stream) {
          const content = chunk.content as string;
          if (content) {
            fullResponse += content;
            onToken(content, "content");
          }
        }
      }

      console.log(
        `\n--- [LLM Output] ---\n${fullResponse.substring(
          0,
          200
        )}...\n--------------------\n`
      );
      return fullResponse;
    } catch (error) {
      console.error("LLM Stream With History Error:", error);
      const errorMsg = `Error: ${
        error instanceof Error ? error.message : String(error)
      }`;
      onToken(errorMsg, "content");
      return errorMsg;
    }
  }

  /**
   * ç”Ÿæˆå¯¹è¯æ‘˜è¦
   * @param messages è¦æ‘˜è¦çš„å¯¹è¯å†å²
   * @returns Promise<string> æ‘˜è¦å†…å®¹
   */
  public async generateSummary(messages: Message[]): Promise<string> {
    if (!this.chatModel || messages.length === 0) {
      return "";
    }

    try {
      // æ„å»ºæ‘˜è¦æç¤º
      let conversationText = "";
      for (const msg of messages) {
        const roleLabel = msg.role === "user" ? "ç”¨æˆ·" : "Lumina";
        conversationText += `${roleLabel}ï¼š${msg.content}\n`;
      }

      const summaryPrompt = `è¯·å°†ä»¥ä¸‹å¯¹è¯æ€»ç»“ä¸ºä¸€æ®µç®€çŸ­çš„æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼š\n\n${conversationText}\n\næ‘˜è¦ï¼š`;

      const response = await this.chatModel.invoke([
        new SystemMessage(
          "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç®€æ´å‡†ç¡®åœ°æ¦‚æ‹¬å¯¹è¯å†…å®¹ã€‚"
        ),
        new HumanMessage(summaryPrompt),
      ]);

      return response.content as string;
    } catch (error) {
      console.error("Summary Generation Error:", error);
      return "";
    }
  }

  /**
   * æ›´æ–°æ‘˜è¦ï¼šåˆå¹¶æ—§æ‘˜è¦å’Œæ–°å¯¹è¯
   * @param currentSummary å½“å‰æ‘˜è¦
   * @param newMessages éœ€è¦åˆå¹¶çš„æ–°å¯¹è¯
   * @returns Promise<string> æ›´æ–°åçš„æ‘˜è¦
   */
  public async updateSummary(
    currentSummary: string,
    newMessages: Message[]
  ): Promise<string> {
    if (!this.chatModel || newMessages.length === 0) {
      return currentSummary;
    }

    try {
      let conversationText = "";
      for (const msg of newMessages) {
        const roleLabel = msg.role === "user" ? "ç”¨æˆ·" : "Lumina";
        conversationText += `${roleLabel}ï¼š${msg.content}\n`;
      }

      const summaryPrompt = `è¿™æ˜¯ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ï¼š
"${currentSummary}"

è¿™æ˜¯éšåå‘ç”Ÿçš„å¯¹è¯ï¼š
${conversationText}

è¯·æ›´æ–°æ‘˜è¦ï¼ŒåŒ…å«ä¹‹å‰çš„å…³é”®ä¿¡æ¯å’Œæ–°çš„å¯¹è¯å†…å®¹ï¼Œä¿æŒåœ¨150å­—ä»¥å†…ï¼š`;

      const response = await this.chatModel.invoke([
        new SystemMessage("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ã€‚"),
        new HumanMessage(summaryPrompt),
      ]);

      return response.content as string;
    } catch (error) {
      console.error("Summary Update Error:", error);
      return currentSummary;
    }
  }
}

export const llmService = new LLMService();
