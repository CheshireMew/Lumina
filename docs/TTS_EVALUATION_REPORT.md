# TTS 情感表现力评估报告

## 测试结果总结

### Edge TTS 测试（已完成）

**测试方法**：
1. ✅ 统一音色测试（`test_tts_unified.py`）
2. ✅ 直接 SSML 验证（`verify_edge_tts_ssml.py`） - 绕过我们的封装

**测试音频位置**：
- `python_backend/unified_test/` - 8个情感样本
- `python_backend/ssml_verification/` - 4个直接调用样本

**用户反馈**：
- ❌ `whispering` 音量没有明显变小
- ❌ 听不出不同情感之间的明显区别

**结论**：
> **Edge TTS 的情感表现力不满足需求**
> 
> 即使 SSML 格式正确，Edge TTS 本身的情感变化也非常微妙，
> 对于需要**明显**情感表现的应用场景不适合。

---

## 推荐方案：GPT-SoVITS

### 为什么选择 GPT-SoVITS？

| 特性 | Edge TTS | GPT-SoVITS |
|------|---------|-----------|
| 情感丰富度 | ⭐⭐ (微妙) | ⭐⭐⭐⭐⭐ (极其自然) |
| 本地部署 | ❌ 云端 | ✅ 完全本地 |
| 延迟 | 低 (~200ms) | 中 (~500ms 首字) |
| 配置难度 | 简单 | 中等 |
| 声音克隆 | ❌ | ✅ (5-10秒参考音频) |
| 显存需求 | 0 | 4-6GB |
| 情感控制 | 预设样式 | 参考音频驱动 |

### GPT-SoVITS 优势

1. **情感极其自然** ⭐⭐⭐⭐⭐
   - 通过参考音频直接克隆情感
   - 可以模仿任何语气：开心、悲伤、生气、害羞等
   - 情感变化明显可感知

2. **声音一致性**
   - 可以克隆特定角色的声音（如你的 Hiyori）
   - 保持角色声音特征的同时变换情感

3. **本地隐私**
   - 完全离线运行
   - 无需依赖外部服务

### 系统需求

**最低配置**：
- GPU: 4GB 显存（GTX 1650 级别）
- RAM: 8GB
- 存储: ~5GB（模型 + 运行时）

**推荐配置**：
- GPU: 6GB+ 显存（RTX 2060 及以上）
- RAM: 16GB

**你的配置**：
- 应该满足要求（有 GPU 用于 Whisper large-v3）

---

## 实现方案

### 方案 A: 完全替换 Edge TTS（推荐）

**优点**：
- 统一的高质量情感输出
- 简化架构

**缺点**：
- 需要时间配置和训练参考音频

### 方案 B: 混合模式

**Edge TTS** → 快速回复（普通对话）
**GPT-SoVITS** → 情感回复（需要表达强烈情感时）

**优点**：
- 保留 Edge TTS 的低延迟优势
- 在需要时使用高质量情感

**实现**：
- 添加 `/tts/synthesize/gptsovits` 端点
- 前端根据情感标签选择引擎

---

## 下一步行动

### 如果选择部署 GPT-SoVITS：

#### 步骤 1: 安装和配置
1. 克隆 GPT-SoVITS 仓库
2. 安装依赖（约 10 分钟）
3. 下载预训练模型（~3GB）

#### 步骤 2: 准备参考音频
需要为每种情感准备 5-10 秒的参考音频：
- `happy.wav` - 开心的语音
- `sad.wav` - 悲伤的语音
- `angry.wav` - 生气的语音
- `neutral.wav` - 平静的语音

**来源选项**：
1. 自己录制（最佳）
2. 使用动漫/游戏角色语音（版权考虑）
3. 使用公开的语音数据集

#### 步骤 3: 集成到 Lumina
1. 创建 `gptsovits_engine.py`
2. 添加 API 端点
3. 前端适配

**预计时间**：2-3 小时（首次配置）

---

## 其他备选方案

### VITS（轻量级）

**优点**：
- 比 GPT-SoVITS 更轻量
- 延迟更低

**缺点**：
- 情感控制不如 GPT-SoVITS
- 需要训练特定角色模型

### Bark（多语言）

**优点**：
- 支持笑声、叹气等非语音声音
- 开箱即用

**缺点**：
- 显存需求高（8GB+）
- 生成速度慢

---

## 决策建议

### 如果你：

**✅ 重视情感表现 + 愿意花时间配置**
→ **推荐：GPT-SoVITS**

**✅ 想要快速方案 + 情感要求不极高**
→ **保留 Edge TTS**，接受其局限性

**✅ 想要平衡**
→ **混合模式**（Edge TTS + GPT-SoVITS）

---

## 我的建议

基于你的反馈（听不出 Edge TTS 情感区别），我建议：

1. **先播放验证脚本生成的音频**
   - `python_backend/ssml_verification/direct_whispering.mp3`
   - 这是直接调用 Edge TTS 的结果，排除我们代码的问题

2. **如果确认 Edge TTS 不满足需求**
   - 我帮你部署 **GPT-SoVITS**
   - 准备好 GPU 和大约 2-3 小时时间

3. **过渡方案**
   - 暂时保留 Edge TTS 用于基本 TTS
   - 情感功能暂时通过 Live2D 动作表达（你已有的 `emotion_map.json`）
   - 等 GPT-SoVITS 配置好后再启用语音情感

---

**你想继续使用 Edge TTS 还是部署 GPT-SoVITS？**
