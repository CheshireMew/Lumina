# 本地情感 TTS 最终决策:GPT-SoVITS vs CosyVoice

针对你的核心需求:"**不要模仿特定角色,只要情感表达**",这里有两个终极选项。

| 维度 | **完美交互方案 (CosyVoice)** | **稳健落地方案 (GPT-SoVITS)** |
| :--- | :--- | :--- |
| **控制方式** | **文字指令** (Install-Free)<br>输入: `[happy] 大家好` | **参考音频** (Reference-Based)<br>输入: 调用 `happy.wav` |
| **原理** | 像 ChatGPT 一样,直接"听懂"情感指令 | 像鹦鹉学舌,模仿给它的那段音频的语气 |
| **你的体验** | ✅ **极好**:直接写 prompt 即可 | 🟡 **一般**:需要先准备个音频库 |
| **显存风险** | 🔴 **高**:即便用最小的 300M 模型,加上 KV Cache 也很容易和 LLM 抢显存 (OOM)。 | 🟢 **低**:架构极简,显存占用非常稳定。 |
| **推荐结论** | **可以尝试,但如果爆显存就得放弃** | **绝对能跑,永远的保底方案** |

---

## 💡 我的建议

**虽然 GPT-SoVITS 稳,但 CosyVoice 的交互方式(直接写`<happy>`)确实更适合你的需求。**

鉴于 CosyVoice 有一个 **300M SFT** 的超小模型(ModelScope 上有),我们可以**赌一把**:
尝试部署 **CosyVoice-300M-SFT**。

*   **优点**:如果能在你的 8GB 显卡上跑通,你就能通过文本指令控制情感(无需准备音频文件)。
*   **退路**:如果跑不起来(卡顿/报错),我们立刻切回 GPT-SoVITS。

**你想赌一把 CosyVoice (方案 A),还是直接求稳上 GPT-SoVITS (方案 B)?**
