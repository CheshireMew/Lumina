# å‚è€ƒé¡¹ç›®éŸ³é¢‘æ•è·æ–¹æ¡ˆåˆ†ææŠ¥å‘Š

## ğŸ¯ Executive Summary

ç»è¿‡å¯¹ 11 ä¸ªåŒç±»é¡¹ç›®çš„æ·±å…¥ç ”ç©¶ï¼Œå‘ç°äº†**ä¸‰ç§ä¸»æµéŸ³é¢‘æ¶æ„**ï¼š

1. **çº¯å‰ç«¯æ¶æ„**ï¼ˆ6ä¸ªé¡¹ç›®ï¼‰ï¼šä½¿ç”¨ `getUserMedia` + `MediaRecorder` + VAD.js
2. **çº¯åç«¯æ¶æ„**ï¼ˆ3ä¸ªé¡¹ç›®ï¼‰ï¼šä½¿ç”¨ `pyaudio`/`sounddevice` + `webrtcvad`
3. **æ··åˆæ¶æ„**ï¼ˆ2ä¸ªé¡¹ç›®ï¼‰ï¼šå‰ç«¯é‡‡é›† + åç«¯VADå¤„ç†

## ğŸ“Š é¡¹ç›®åˆ†ç±»ç»Ÿè®¡

### âœ… çº¯å‰ç«¯éŸ³é¢‘æ•è·ï¼ˆ6ä¸ªé¡¹ç›®ï¼‰
- **N.E.K.O** - å®Œæ•´å‰ç«¯VAD + AudioWorklet
- **MoeChat** - MediaRecorder + pysileroåç«¯VADï¼ˆWebSocketï¼‰
- **super-agent-party** - @ricky0123/vad-reactï¼ˆSilero VADï¼‰
- **my-neuro** - AudioWorklet + ASR Processor
- **ai_virtual_mate_web** - getUserMediaåŸºç¡€æ–¹æ¡ˆ
- **Lunar-Astral-Agents** - å‰ç«¯éŸ³é¢‘æµ

### âœ… çº¯åç«¯éŸ³é¢‘æ•è·ï¼ˆ3ä¸ªé¡¹ç›®ï¼‰
- **Live2D-Virtual-Girlfriend** â­ **é‡ç‚¹å­¦ä¹ å¯¹è±¡**
- **NagaAgent** ï¼ˆä»…ä¾èµ–æ£€æŸ¥ï¼ŒæœªæŸ¥åˆ°å…·ä½“å®ç°ï¼‰
- **ZcChat** - Qt C++ `QMediaRecorder`

### â“ æ¶æ„ä¸æ˜ç¡®ï¼ˆ2ä¸ªé¡¹ç›®ï¼‰
- **deepseek-Lunasia-2.0** - æœªæ‰¾åˆ°README
- **nana** - æœªæ‰¾åˆ°æ˜ç¡®éŸ³é¢‘ä»£ç 

---

## ğŸ”¬ æ·±åº¦åˆ†æï¼šLive2D-Virtual-Girlfriendï¼ˆæœ€ä½³å‚è€ƒï¼‰

### æ¶æ„ç‰¹ç‚¹
```
åç«¯å®Œå…¨æ§åˆ¶éŸ³é¢‘æµï¼šPyAudio æ•è· â†’ webrtcvad å®æ—¶VAD â†’ SenseVoice STT
```

### æ ¸å¿ƒä»£ç æ¶æ„ï¼ˆasr.pyï¼‰

#### 1. VAD å¤„ç†ç±»
```python
class RealTimeVAD:
    def __init__(self, aggressiveness=3, sample_rate=16000):
        self.vad = webrtcvad.Vad(aggressiveness)
        self.sample_rate = 16000
        self.frame_duration = 30  # ms
        self.frame_length = int(16000 * 30 / 1000)  # 480 samples
        
        # æ»‘åŠ¨çª—å£ç¼“å†²åŒºï¼š10å¸§å†å²
        self.speech_buffer = collections.deque(maxlen=10)
    
    def process_frame(self, frame_data):
        is_speech = self.vad.is_speech(frame_data, self.sample_rate)
        self.speech_buffer.append(is_speech)
        
        speech_ratio = sum(self.speech_buffer) / len(self.speech_buffer)
        
        # çŠ¶æ€æœºï¼šsilence â†’ speech_start â†’ speech_continue â†’ speech_end
        if not self.is_speaking and speech_ratio > 0.5:
            return "speech_start"
        elif self.is_speaking and speech_ratio < 0.3:
            return "speech_end"
        elif self.is_speaking:
            return "speech_continue"
        else:
            return "silence"
```

**ğŸ’¡ å…³é”®è®¾è®¡äº®ç‚¹**ï¼š
- âœ… **æ»‘åŠ¨çª—å£å¹³æ»‘**ï¼šç”¨ 10å¸§å†å²è®¡ç®—è¯­éŸ³æ¯”ä¾‹ï¼Œé¿å…å•å¸§è¯¯åˆ¤
- âœ… **é˜ˆå€¼åŒé—¨é™**ï¼š`0.5` å¯åŠ¨ã€`0.3` ç»“æŸï¼Œé˜²æŠ–æŠ—å¹²æ‰°
- âœ… **30mså¸§å¤§å°**ï¼šwebrtcvad æ ‡å‡†å¸§é•¿ï¼ˆ10/20/30msï¼‰

#### 2. éŸ³é¢‘æ•è·å¾ªç¯
```python
def speech_recognition(web=False):
    CHUNK = int(16000 * 0.03)  # 30ms = 480 samples
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    
    # é¢„ç¼“å†²åŒºï¼šä¿ç•™è¯­éŸ³å‰ 0.5s
    PRE_BUFFER_SECONDS = 0.5
    PRE_BUFFER_SIZE = int(RATE / CHUNK * PRE_BUFFER_SECONDS)
    
    p = pyaudio.PyAudio()
    stream = p.open(
        format=FORMAT,
        channels=CHANNELS,
        rate=RATE,
        input=True,
        frames_per_buffer=CHUNK
    )
    
    while True:
        frames = []
        pre_buffer = deque(maxlen=PRE_BUFFER_SIZE)
        
        while True:
            data = stream.read(CHUNK)
            vad_result = vad_processor.process_frame(data)
            
            if vad_result == "speech_start":
                # ğŸ“Œ å…³é”®ï¼šæ£€æµ‹åˆ°è¯­éŸ³åï¼Œå°†é¢„ç¼“å†²åŒºçš„å†å²éŸ³é¢‘ä¹ŸåŠ å…¥
                frames.extend(pre_buffer)
                frames.append(data)
                
            elif vad_result == "speech_continue":
                frames.append(data)
                
            elif has_speech and vad_result == "silence":
                break
            
            # å§‹ç»ˆç»´æŠ¤é¢„ç¼“å†²åŒº
            pre_buffer.append(data)
        
        # ä¿å­˜ä¸º WAV å¹¶è°ƒç”¨ STT
        wf = wave.open('temp/temp.wav', 'wb')
        wf.writeframes(b''.join(frames))
```

**ğŸ’¡ å…³é”®è®¾è®¡äº®ç‚¹**ï¼š
- âœ… **é¢„ç¼“å†²åŒº**ï¼šæ•è·è¯­éŸ³èµ·å§‹å‰ 0.5sï¼Œé¿å…ä¸¢å¤±å¼€å¤´éŸ³èŠ‚
- âœ… **å¾ªç¯æ£€æµ‹**ï¼šæŒç»­ç›‘å¬ï¼Œæ— éœ€æ‰‹åŠ¨è§¦å‘
- âœ… **è®¾å¤‡éš”ç¦»**ï¼š`pyaudio.open(input=True)` ç›´æ¥æŒ‡å®šè¾“å…¥è®¾å¤‡ç´¢å¼•

---

## ğŸ†š æ–¹æ¡ˆå¯¹æ¯”ï¼šå‰ç«¯ vs åç«¯

| ç»´åº¦ | çº¯å‰ç«¯ | çº¯åç«¯ï¼ˆpyaudio+webrtcvadï¼‰ | **æˆ‘ä»¬å½“å‰** |
|------|--------|---------------------------|-----------|
| **è®¾å¤‡éš”ç¦»** | âŒ æ··æ·†éº¦å…‹é£/å›ç¯ | âœ… ç²¾ç¡®é€‰æ‹©ç‰©ç†è®¾å¤‡ | âŒ å‰ç«¯VAD |
| **VADå‡†ç¡®æ€§** | âš ï¸ Sileroæ¨¡å‹å¤§ | âœ… webrtcvadä¸“ä¸šè½»é‡ | âš ï¸ @ricky0123/vad-react |
| **å»¶è¿Ÿ** | ~100-300ms | ~50-100ms | ~200ms |
| **è·¨å¹³å°** | âœ… Webé€šç”¨ | âš ï¸ éœ€æœ¬åœ°Python | âœ… Electronå…¼å®¹ |
| **å¤æ‚åº¦** | ä½ | ä¸­ | å½“å‰ï¼šä½ |
| **å›ç¯é—®é¢˜** | âŒ **æ— æ³•å½»åº•è§£å†³** | âœ… **å®Œå…¨éš”ç¦»** | âŒ **å½“å‰ç—›ç‚¹** |

### âš ï¸ å‰ç«¯æ–¹æ¡ˆçš„è‡´å‘½ç¼ºé™·ï¼ˆä¸ºä»€ä¹ˆè¦è¿ç§»ï¼‰

#### N.E.K.O é¡¹ç›®çš„ç—›è‹¦ç»éªŒ
```javascript
// app.js:1105-1112 - ä»–ä»¬ä¹Ÿå°è¯•ç¦ç”¨å›å£°æ¶ˆé™¤ï¼Œä½†æ— æ•ˆ
const baseAudioConstraints = {
    noiseSuppression: false,
    echoCancellation: true,   // âš ï¸ å³ä½¿å¼€å¯ï¼Œç³»ç»Ÿä»å¯èƒ½å›ç¯
    autoGainControl: true,
    channelCount: 1
};
```

**é—®é¢˜åˆ†æ**ï¼š
1. `getUserMedia` æ— æ³•åŒºåˆ†"éº¦å…‹é£A"å’Œ"ç«‹ä½“å£°æ··éŸ³ï¼ˆç³»ç»Ÿå›ç¯ï¼‰"
2. Windowsé»˜è®¤è®¾å¤‡å¯èƒ½è¢«å…¶ä»–åº”ç”¨ç¯¡æ”¹
3. æµè§ˆå™¨çš„ `deviceId` ä¸ç¨³å®šï¼ˆè®¾å¤‡é‡æ–°æ’æ‹”åä¼šå˜ï¼‰

#### MoeChat çš„æŠ˜è¡·æ–¹æ¡ˆ
```javascript
// moechat_core.js:319-322 - ä»–ä»¬é€‰æ‹©å‰ç«¯é‡‡é›†+åç«¯VAD
navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
    mediaRecorder = new MediaRecorder(stream);
    // WebSocket å‘é€åˆ°åç«¯ pysilero VAD
});
```

**ä»ç„¶æ— æ³•è§£å†³**ï¼šå‰ç«¯é‡‡é›†é˜¶æ®µå·²ç»æ··å…¥äº†ç³»ç»ŸéŸ³é¢‘

---

## ğŸ“‹ æŠ€æœ¯æ ˆå¯¹æ¯”

### åç«¯VADåº“é€‰æ‹©

| åº“ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é¡¹ç›®ä½¿ç”¨ |
|----|------|------|----------|
| **webrtcvad** | âœ… è¶…è½»é‡ï¼ˆGoogleå®˜æ–¹ï¼‰<br>âœ… å»¶è¿Ÿæä½<br>âœ… CPUå ç”¨å° | âš ï¸ ä»…æ”¯æŒ16kHz<br>âš ï¸ åŠŸèƒ½å•ä¸€ | Live2D-VG â­ |
| **pysilero** | âœ… å‡†ç¡®ç‡æ›´é«˜<br>âœ… æ”¯æŒå¤šé‡‡æ ·ç‡ | âŒ æ¨¡å‹è¾ƒå¤§ï¼ˆ~2MBï¼‰<br>âŒ éœ€ONNX Runtime | MoeChat |
| **Silero VAD (JS)** | âœ… çº¯å‰ç«¯<br>âœ… ONNX Web | âŒ å›ç¯é—®é¢˜<br>âŒ åŠ è½½æ…¢ | N.E.K.O, super-agent |

### éŸ³é¢‘æ•è·åº“é€‰æ‹©

| åº“ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é¡¹ç›®ä½¿ç”¨ |
|----|------|------|----------|
| **pyaudio** | âœ… æˆç†Ÿç¨³å®š<br>âœ… è·¨å¹³å° | âš ï¸ å®‰è£…ä¾èµ–è¾ƒé‡<br>âš ï¸ éœ€PortAudio | Live2D-VG â­ |
| **sounddevice** | âœ… ç°ä»£API<br>âœ… æ›´å¥½çš„é”™è¯¯å¤„ç† | âš ï¸ ä¹Ÿéœ€PortAudio | NagaAgent |
| **getUserMedia** | âœ… é›¶ä¾èµ– | âŒ **æ— æ³•éš”ç¦»è®¾å¤‡** | å¤§å¤šæ•°Webé¡¹ç›® |

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### âœ… é‡‡çº³å»ºè®®

#### 1. **ä½¿ç”¨ `sounddevice` è€Œé `pyaudio`**
   - âœ… æ›´ç°ä»£çš„APIï¼ˆ2015+ vs 2006ï¼‰
   - âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œè®¾å¤‡æšä¸¾
   - âœ… æ”¯æŒå¼‚æ­¥å›è°ƒï¼ˆå¯é€‰ï¼‰

#### 2. **ä¿ç•™ `webrtcvad`**
   - âœ… Live2D-Virtual-Girlfriend éªŒè¯è¿‡çš„æ–¹æ¡ˆ
   - âœ… èµ„æºå ç”¨æå°ï¼Œé€‚åˆå®æ—¶åœºæ™¯

#### 3. **å€Ÿé‰´é¢„ç¼“å†²åŒºè®¾è®¡**
```python
# å‚è€ƒ Live2D-VG çš„è®¾è®¡
PRE_BUFFER_SECONDS = 0.5  # ä¿ç•™è¯­éŸ³å‰0.5ç§’
```

#### 4. **æ»‘åŠ¨çª—å£çŠ¶æ€æœº**
```python
speech_buffer = collections.deque(maxlen=10)
speech_ratio = sum(speech_buffer) / len(speech_buffer)

if speech_ratio > 0.5:  # å¯åŠ¨é˜ˆå€¼
    trigger_speech_start()
elif speech_ratio < 0.3:  # åœæ­¢é˜ˆå€¼
    trigger_speech_end()
```

---

## âš ï¸ éœ€è¦æ³¨æ„çš„å‘

### 1. **PyAudio vs SoundDevice ä¾èµ–é—®é¢˜**
```bash
# PyAudio åœ¨ Windows ä¸Šå®‰è£…å›°éš¾
pip install pyaudio  # âŒ å¯èƒ½å¤±è´¥

# SoundDevice æ›´å‹å¥½
pip install sounddevice  # âœ… æ¨è
```

### 2. **è®¾å¤‡ç´¢å¼•æŒä¹…åŒ–**
```python
# âŒ é”™è¯¯ï¼šç›´æ¥å­˜å‚¨è®¾å¤‡ç´¢å¼•
config['device_index'] = 2

# âœ… æ­£ç¡®ï¼šå­˜å‚¨è®¾å¤‡å”¯ä¸€æ ‡è¯†
import sounddevice as sd
devices = sd.query_devices()
config['device_name'] = devices[2]['name']  # "Microphone (Realtek)"

# å¯åŠ¨æ—¶é‡æ–°åŒ¹é…
for i, dev in enumerate(sd.query_devices()):
    if dev['name'] == config['device_name']:
        device_index = i
```

### 3. **WebSocketæ¨é€ç­–ç•¥**
å‚è€ƒ MoeChat çš„è®¾è®¡ï¼š
```javascript
// å‰ç«¯ç›‘å¬ VAD çŠ¶æ€
socket.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    if (data.type === 'vad_status') {
        updateUI(data.status);  // 'listening' | 'thinking' | 'idle'
    }
    
    if (data.type === 'transcription') {
        onSend(data.text);
    }
};
```

---

## ğŸ—ï¸ æ¨èæ¶æ„ï¼ˆåŸºäºç ”ç©¶ç»“è®ºï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Electron Frontend (React)         â”‚
â”‚  - æ— éº¦å…‹é£æƒé™è¯·æ±‚                           â”‚
â”‚  - WebSocket ç›‘å¬ VAD çŠ¶æ€                   â”‚
â”‚  - UI æ˜¾ç¤º listening/thinking                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ WebSocket
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Python Backend (FastAPI)              â”‚
â”‚                                             â”‚
â”‚  sounddevice.InputStream                    â”‚
â”‚       â†“                                     â”‚
â”‚  webrtcvad.Vad.is_speech()                  â”‚
â”‚       â†“                                     â”‚
â”‚  [Pre-Buffer] + [VAD Frames]                â”‚
â”‚       â†“                                     â”‚
â”‚  Whisper STT                                â”‚
â”‚       â†“                                     â”‚
â”‚  WebSocket.send({type: 'transcription'})   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ æ ¸å¿ƒä»£ç å‚è€ƒï¼ˆLive2D-VG æ”¹å†™ç‰ˆï¼‰

```python
import sounddevice as sd
import webrtcvad
import numpy as np
from collections import deque

class AudioManager:
    def __init__(self, device_index=None, sample_rate=16000):
        self.vad = webrtcvad.Vad(aggressiveness=3)
        self.sample_rate = sample_rate
        self.frame_duration_ms = 30
        self.frame_size = int(sample_rate * self.frame_duration_ms / 1000)
        
        self.device_index = device_index
        self.speech_buffer = deque(maxlen=10)
        self.pre_buffer = deque(maxlen=int(0.5 * 1000 / self.frame_duration_ms))  # 0.5s
        self.audio_frames = []
        self.is_speaking = False
    
    def process_frame(self, frame: np.ndarray):
        # Convert float32 [-1,1] to int16 PCM
        pcm = (frame.clip(-1, 1) * 32767).astype(np.int16).tobytes()
        
        # VAD detection
        is_speech = self.vad.is_speech(pcm, self.sample_rate)
        self.speech_buffer.append(is_speech)
        speech_ratio = sum(self.speech_buffer) / len(self.speech_buffer)
        
        # State machine
        if not self.is_speaking and speech_ratio > 0.5:
            self.is_speaking = True
            self.audio_frames.extend(self.pre_buffer)  # Add pre-buffer
            self.audio_frames.append(frame)
            return "speech_start"
        
        elif self.is_speaking and speech_ratio < 0.3:
            self.is_speaking = False
            audio = np.concatenate(self.audio_frames)
            self.audio_frames.clear()
            return "speech_end", audio
        
        elif self.is_speaking:
            self.audio_frames.append(frame)
            return "speech_continue"
        
        else:
            self.pre_buffer.append(frame)
            return "silence"
    
    def start(self, callback):
        with sd.InputStream(
            device=self.device_index,
            samplerate=self.sample_rate,
            channels=1,
            dtype='float32',
            blocksize=self.frame_size,
            callback=lambda indata, frames, time, status: callback(indata[:, 0])
        ):
            input("Press Enter to stop...")  # é˜»å¡ä¸»çº¿ç¨‹
```

---

## ğŸ“ ç»“è®ºä¸å»ºè®®

### âœ… å¼ºçƒˆæ¨èè¿ç§»åˆ°åç«¯VAD
ç»è¿‡å¯¹ 11 ä¸ªé¡¹ç›®çš„ç ”ç©¶ï¼Œ**Live2D-Virtual-Girlfriend** çš„çº¯åç«¯æ–¹æ¡ˆæ˜¯**æœ€æˆç†Ÿã€æœ€å¯é **çš„è§£å†³æ–¹æ¡ˆï¼š

1. **å½»åº•è§£å†³å›ç¯é—®é¢˜**ï¼ˆN.E.K.Oç­‰å‰ç«¯é¡¹ç›®æ— æ³•è§£å†³ï¼‰
2. **æŠ€æœ¯æ ˆå·²éªŒè¯**ï¼ˆwebrtcvad + pyaudio/sounddeviceï¼‰
3. **æ€§èƒ½ä¼˜ç§€**ï¼ˆå»¶è¿Ÿä½ã€èµ„æºå ç”¨å°ï¼‰

### ğŸ› ï¸ å®æ–½è·¯å¾„
1. æ·»åŠ  `sounddevice` ä¾èµ–ï¼ˆæ¯” `pyaudio`  æ›´å‹å¥½ï¼‰
2. å®Œå…¨å¤ç”¨ `webrtcvad`ï¼ˆå·²åœ¨ requirements.txtï¼‰
3. å€Ÿé‰´ Live2D-VG çš„**é¢„ç¼“å†²åŒº**å’Œ**æ»‘åŠ¨çª—å£**è®¾è®¡
4. WebSocket æ¨é€ VAD çŠ¶æ€ï¼ˆå‚è€ƒ MoeChatï¼‰

### âœ… é¿å…çš„é”™è¯¯
- âŒ ä¸è¦å°è¯•"å‰ç«¯é‡‡é›†+åç«¯VAD"ï¼ˆMoeChatçš„æŠ˜è¡·æ–¹æ¡ˆä»æ— æ³•è§£å†³å›ç¯ï¼‰
- âŒ ä¸è¦ä½¿ç”¨ `pysilero`ï¼ˆè¿‡é‡ï¼Œwebrtcvad è¶³å¤Ÿï¼‰
- âŒ ä¸è¦ç›´æ¥å­˜å‚¨è®¾å¤‡ç´¢å¼•ï¼ˆéœ€å­˜å‚¨è®¾å¤‡åç§°å¹¶åŠ¨æ€åŒ¹é…ï¼‰
